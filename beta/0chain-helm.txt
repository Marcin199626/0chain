---
# Source: 0chain/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: RELEASE-NAME-0chain
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: 0chain/templates/0dns-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: RELEASE-NAME-0chain-0dns
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm-0dns
data:
  0dns.yaml: |-
    version: 1.0

    logging:
      level: "info"
      console: true # printing log to console is only supported in development mode

    server_chain:
      id: "0afc093ffb509f059c55478bc1a60351cef7b4e9c008a53a6cc8241ca8617dfe"
      signature_scheme: "bls0chain"

    port: 9091
    use_https: false
    use_path: true
    mongo:
      url: mongodb://RELEASE-NAME-0chain-mongo:27017
      db_name: block-recorder
      pool_size: 2

    worker:
      magic_block_worker: 5 # in seconds
---
# Source: 0chain/templates/blobber-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: RELEASE-NAME-0chain-blobber
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm-blobber
data:
  0chain_blobber.yaml: |-
    version: 1.0

    logging:
      level: "debug"
      console: false # printing log to console is only supported in development mode

    # for testing
    #  500 MB - 536870912
    #    1 GB - 1073741824
    #    2 GB - 2147483648
    #    3 GB - 3221225472
    #  100 GB - 107374182400
    capacity: 107374182400 # 100 GB bytes total blobber capacity
    read_price: 0.01     # token / GB for reading
    write_price: 0.01    # token / GB / time_unit for writing
    # the time_unit configured in Storage SC and can be given using
    #
    #     ./zbox sc-config
    #
    # min_lock_demand is value in [0; 1] range; it represents number of tokens the
    # blobber earned even if a user will not read or write something
    # to an allocation; the number of tokens will be calculated by the following
    # formula (regarding the time_unit and allocation duration)
    #
    #     allocation_size * write_price * min_lock_demand
    #
    min_lock_demand: 0.1
    # max_offer_duration restrict long contracts where,
    # in the future, prices can be changed
    max_offer_duration: 744h # 31 day
    challenge_completion_time: 2m # duration to complete a challenge
    # these timeouts required by blobber to check client pools, perform
    # a task and redeem tokens, it should be big enough
    read_lock_timeout: 1m
    write_lock_timeout: 1m
    max_file_size: 10485760 #10MB

    # update_allocations_interval used to refresh known allocation objects from SC
    update_allocations_interval: 1m

    # delegate wallet (must be set)
    delegate_wallet: '75056ed97e5a44cb154d4d2e897568fc9b46d2fa157d342bff4a6be684447646'
    # min stake allowed, tokens
    min_stake: 1.0
    # max stake allowed, tokens
    max_stake: 100.0
    # maximum allowed number of stake holders
    num_delegates: 50
    service_charge: 0.30

    block_worker: http://RELEASE-NAME-0chain-0dns:9091
    handlers:
      rate_limit: 10

    server_chain:
      id: "0afc093ffb509f059c55478bc1a60351cef7b4e9c008a53a6cc8241ca8617dfe"
      owner: "edb90b850f2e7e7cbd0a1fa370fdcc5cd378ffbec95363a7bc0e5a98b8ba5759"
      genesis_block:
        id: "ed79cae70d439c11258236da1dfa6fc550f7cc569768304623e8fbd7d70efae4"
      signature_scheme: "bls0chain"

    contentref_cleaner:
      frequency: 30
      tolerance: 3600
    openconnection_cleaner:
      frequency: 30
      tolerance: 3600
    writemarker_redeem:
      frequency: 10
      num_workers: 5
    readmarker_redeem:
      frequency: 10
      num_workers: 5
    challenge_response:
      frequency: 10
      num_workers: 5
      max_retries: 20
    db:
      name: blobber_meta
      user: blobber_user
      password: blobber
      host: postgres
      port: 5432

    minio:
      # Enable or disable minio backup service
      start: true
      # The frequency at which the worker should look for files, Ex: 3600 means it will run every 3600 seconds
      worker_frequency: 1800 # In Seconds
      # Use SSL for connection or not
      use_ssl: true

    cold_storage:
      # Minimum file size to be considered for moving to cloud
      min_file_size: 1048576 #in bytes
      # Minimum time for which file is not updated or not used
      file_time_limit_in_hours: 1 # 720 #in hours
      # Number of files to be queried and processed at once
      job_query_limit: 100
      # Capacity filled in bytes after which the cloud backup should start work
      start_capacity_size: 10485760 # 536870912 # 500MB
      # Delete local copy once the file is moved to cloud
      delete_local_copy: true
      # Delete cloud copy if the file is deleted from the blobber by user/other process
      delete_cloud_copy: true
    # integration tests related configurations
    integration_tests:
      # address of the server
      address: host.docker.internal:15210
      # lock_interval used by nodes to request server to connect to blockchain
      # after start
      lock_interval: 1s

  0chain_validator.yaml: |-
    version: 1.0

    # delegate wallet (must be set)
    delegate_wallet: '75056ed97e5a44cb154d4d2e897568fc9b46d2fa157d342bff4a6be684447646'
    # min stake allowed, tokens
    min_stake: 1.0
    # max stake allowed, tokens
    max_stake: 100.0
    # maximum allowed number of stake holders
    num_delegates: 50
    # service charge of related blobber
    service_charge: 0.30


    block_worker: http://RELEASE-NAME-0chain-0dns:9091
    handlers:
      rate_limit: 10 # 10 per second

    logging:
      level: "info"
      console: false # printing log to console is only supported in development mode

    server_chain:
      id: "0afc093ffb509f059c55478bc1a60351cef7b4e9c008a53a6cc8241ca8617dfe"
      owner: "edb90b850f2e7e7cbd0a1fa370fdcc5cd378ffbec95363a7bc0e5a98b8ba5759"
      genesis_block:
        id: "ed79cae70d439c11258236da1dfa6fc550f7cc569768304623e8fbd7d70efae4"
      network:
        relay_time: 100 # milliseconds
      signature_scheme: "bls0chain"

  minio_config.txt: |
    play.min.io
    Q3AM3UQ867SPQQA43P2F
    zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG
    mytestbucket
    us-east-1

  b0bnode0_keys.txt: |
    de52c0a51872d5d2ec04dbc15a6f0696cba22657b80520e1d070e72de64c9b04e19ce3223cae3c743a20184158457582ffe9c369ca9218c04bfe83a26a62d88d
    17fa2ab0fb49249cb46dbc13e4e9e6853af8b1506e48d84c03e5e92f6348bb1d
    # RELEASE-NAME-0chain-blobber-0
    # 5051

  b0bnode1_keys.txt: |
    e4dc5262ed8e20583e3293f358cc21aa77c2308ea773bab8913670ffeb5aa30d7e2effbce51f323b5b228ad01f71dc587b923e4aab7663a573ece5506f2e3b0e
    4b6d9c5f7b0386e36b212324ea52f5ff17a9ed1338ca901d7f7fa7637159a912
    # RELEASE-NAME-0chain-blobber-1
    # 5051
---
# Source: 0chain/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: RELEASE-NAME-0chain
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
data:
  0chain.yaml: |
    version: 1.0

    logging:
      level: "debug"
      console: true # printing log to console is only supported in development mode
      goroutines: false
      memlog: false

    development:
      state: true
      dkg: true
      view_change: false
      smart_contract:
        storage: true
        faucet: true
        zrc20: true
        interest: true
        miner: true
        multisig: true
        vesting: true
      txn_generation:
        wallets: 50
        max_transactions: 0
        max_txn_fee: 0
        min_txn_fee: 0
        max_txn_value: 10000000000
        min_txn_value: 100
      faucet:
        refill_amount: 1000000000000000

    zerochain:
      id: "0afc093ffb509f059c55478bc1a60351cef7b4e9c008a53a6cc8241ca8617dfe"
      decimals: 10
      genesis_block:
        id: "ed79cae70d439c11258236da1dfa6fc550f7cc569768304623e8fbd7d70efae4"

    server_chain:
      id: "0afc093ffb509f059c55478bc1a60351cef7b4e9c008a53a6cc8241ca8617dfe"
      owner: "edb90b850f2e7e7cbd0a1fa370fdcc5cd378ffbec95363a7bc0e5a98b8ba5759"
      decimals: 10
      tokens: 200000000
      genesis_block:
        id: "ed79cae70d439c11258236da1dfa6fc550f7cc569768304623e8fbd7d70efae4"
      block:
        min_block_size: 1
        max_block_size: 10
        max_byte_size: 1638400
        generators: 3
        replicators: 0
        generation:
          timeout: 15
          retry_wait_time: 5 #milliseconds
        proposal:
          max_wait_time: 180 # milliseconds
          wait_mode: static # static or dynamic
        consensus:
          threshold_by_count: 60 # percentage
          threshold_by_stake: 0 # percent
        sharding:
          min_active_sharders: 25 # percentage
          min_active_replicators: 25 # percentage
        validation:
          batch_size: 1000
        reuse_txns: false
        storage:
          provider: blockstore.FSBlockStore # blockstore.FSBlockStore or blockstore.BlockDBStore
      round_range: 10000000
      round_timeouts:
        softto_min: 3000 #in miliseconds
        softto_mult: 3 #multiples of mean network time (mnt)  softto = max{softo_min, softto_mult * mnt}
        round_restart_mult: 2 #number of soft timeouts before round is restarted
        round_timeout_mult: 2 # increase round timeout by this multiplier if need
        round_timeout_max: 16 # max round timeout value can be set
      transaction:
        payload:
          max_size: 98304 # bytes
        timeout: 30 # seconds
      client:
        signature_scheme: bls0chain # ed25519 or bls0chain
        discover: true
      messages:
        verification_tickets_to: all_miners # generator or all_miners
      state:
        prune_below_count: 100 # rounds
      smart_contract:
        timeout: 8000 # milliseconds
      health_check:
        show_counters: true
        deep_scan:
          enabled: false
          settle_secs: 30
          window: 0 #Full scan till round 0
          repeat_interval_mins: 3 #minutes
          report_status_mins: 1 #minutes
          batch_size: 50
        proximity_scan:
          enabled: false
          settle_secs: 30
          window: 0 #number of blocks
          repeat_interval_mins: 1 #minutes
          report_status_mins: 1 #minutes
          batch_size: 50
      lfb_ticket:
        rebroadcast_timeout: '15s'  #
        ahead: 5                    #
        fb_fetching_lifetime: '10s' #

    network:
      magic_block_file: config/magicBlock.json
      initial_states: config/initial_state.yaml
      relay_time: 200 # milliseconds
      max_concurrent_requests: 40
      timeout:
        small_message: 1000 # milliseconds
        large_message: 3000 # milliseconds
      large_message_th_size: 5120 # anything greater than this size in bytes
      user_handlers:
        rate_limit: 100000000 # 100 per second
      n2n_handlers:
        rate_limit: 100000000 # 10000 per second

    # delegate wallet is wallet that used to configure node in Miner SC; if its
    # empty, then node ID used
    delegate_wallet: ''
    # % of fees and rewards for generator
    service_charge: 0.10      # [0; 1) of all fees
    # max number of delegate pools allowed by a node in miner SC
    number_of_delegates: 10   # max number of delegate pools
    # min stake pool amount allowed by node; should not conflict with
    # SC min_stake 
    min_stake: 1.0    # tokens
    # max stake pool amount allowed by node; should not conflict with
    # SC max_stake 
    max_stake: 100.0  # tokens

    minio:
      enabled: true # Enable or disable minio backup, Do not enable with deep scan ON
      worker_frequency: 300 # In Seconds, The frequency at which the worker should look for files, Ex: 3600 means it will run every 3600 seconds
      num_workers: 40 # Number of workers to run in parallel, Just to make execution faster we can have mutiple workers running simultaneously
      use_ssl: true # Use SSL for connection or not
      old_block_round_range: 200 # How old the block should be to be considered for moving to cloud
      delete_local_copy: true # Delete local copy of block once it's moved to cloud

    cassandra:
      connection:
        delay: 10 # in seconds
        retries: 10

    # integration tests related configurations
    integration_tests:
      # address of the server
      address: host.docker.internal:15210
      # lock_interval used by nodes to request server to connect to blockchain
      # after start
      lock_interval: 1s

  sc.yaml: |-
    smart_contracts:
      faucetsc: 
        pour_amount: 10000000000
        periodic_limit: 1000000000000
        global_limit: 100000000000000
        individual_reset: 3h # in hours
        global_reset: 48h # in hours
      interestpoolsc:
        min_lock: 10 
        apr: 0.1
        min_lock_period: 1m
        max_mint: 4000000.0

      minersc:
        # miners
        max_n: 100
        min_n: 3
        # sharders
        max_s: 30
        min_s: 1
        # max delegates allowed by SC
        max_delegates: 200 #
        # DKG
        t_percent: .51
        k_percent: .75
        # etc
        min_stake: 0.01 # min stake can be set by a node (boundary for all nodes)
        max_stake: 100.0 # max stake can be set by a node (boundary for all nodes)
        start_rounds: 50
        contribute_rounds: 50
        share_rounds: 50
        publish_rounds: 50
        wait_rounds: 50
        # stake interests, will be declined every epoch
        interest_rate: 0.001 # [0; 1)
        # reward rate for generators, will be declined every epoch
        reward_rate: 1.0 # [0; 1)
        # share ratio is miner/block sharders rewards ratio, for example 0.1
        # gives 10% for miner and rest for block sharders
        share_ratio: 0.10 # [0; 1)
        # reward for a block
        block_reward: 0.7 # tokens
        # max service charge can be set by a generator
        max_charge: 0.5 # %
        # epoch is number of rounds before rewards and interest are decreased
        epoch: 15000000 # rounds
        # decline rewards every new epoch by this value (the block_reward)
        reward_decline_rate: 0.1 # [0; 1), 0.1 = 10%
        # decline interests every new epoch by this value (the interest_rate)
        interest_decline_rate: 0.1 # [0; 1), 0.1 = 10%
        # no mints after miner SC total mints reaches this boundary
        max_mint: 4000000.0 # tokens
        reward_round_frequency: 250

      storagesc:
        # the time_unit is a duration used as divider for a write price; a write
        # price measured in tok / GB / time_unit, where the time_unit is this
        # configuration; for example 1h, 24h (a day), 720h (a month -- 30 days);
        time_unit: '48h'
        min_stake: 0.01 # min stake can be set by a node (boundary for all nodes)
        max_stake: 100.0 # max stake can be set by a node (boundary for all nodes)
        # max_mint
        max_mint: 4000000.0 # tokens, max amount of tokens can be minted by SC
        # min possible allocations size in bytes allowed by the SC
        min_alloc_size: 1024
        # min possible allocation duration allowed by the SC
        min_alloc_duration: '5m'
        # max challenge completion time of a blobber allowed by the SC
        max_challenge_completion_time: '30m'
        # min blobber's offer duration allowed by the SC
        min_offer_duration: '10h'
        # min blobber capacity allowed by the SC
        min_blobber_capacity: 1024
        # users' read pool related configurations
        readpool:
          min_lock: 0 # toekns
          min_lock_period: 1h
          max_lock_period: 8760h
        # users' write pool related configurations
        writepool:
          min_lock: 0 # tokens
          min_lock_period: 2m
          max_lock_period: 8760h
        # stake pool configurations
        stakepool:
          # minimal lock for a delegate pool
          min_lock: 0.1 # tokens
          # interest_rate is tokens earned by a blobber for its stake
          interest_rate: 0.1
          # interest_interval is interval to pay interests for a stake
          interest_interval: 1m
        # validator_reward represents part of blobbers' rewards goes to validators
        validator_reward: 0.025
        # blobber_slash represents blobber's stake penalty when a challenge not
        # passed
        blobber_slash: 0.10
        # max prices for blobbers (tokens per GB)
        max_read_price: 100.0
        max_write_price: 100.0
        #
        # allocation cancellation
        #
        # failed_challenges_to_cancel is number of failed challenges of an
        # allocation to be able to cancel an allocation
        failed_challenges_to_cancel: 20
        # failed_challenges_to_revoke_min_lock is number of failed challenges
        # of a blobber to revoke its min_lock demand back to user; only part
        # not paid yet can go back
        failed_challenges_to_revoke_min_lock: 10
        #
        # challenges
        #
        # enable challenges
        challenge_enabled: true
        # number of challenges for MB per minute
        challenge_rate_per_mb_min: 1
        # max number of challenges can be generated at once
        max_challenges_per_generation: 100
        # max delegates per stake pool allowed by SC
        max_delegates: 200
        max_charge: 0.50
      vestingsc:
        min_lock: 0.01
        min_duration: '2m'
        max_duration: '2h'
        max_destinations: 3
        max_description_length: 20

  b0owner_keys.txt: |
    627eb53becc3d312836bfdd97deb25a6d71f1e15bf3bcd233ab3d0c36300161990d4e2249f1d7747c0d1775ee7ffec912a61bd8ab5ed164fd6218099419c4305
    593f00a0fdf8596589956bca6cfe2e648e2586bba616f417d173c1acda4fce1e
    edb90b850f2e7e7cbd0a1fa370fdcc5cd378ffbec95363a7bc0e5a98b8ba5759

  minio_config.txt: |
    play.min.io
    Q3AM3UQ867SPQQA43P2F
    zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG
    mytestbucket
    us-east-1

  snode0_keys.txt: |
    b9b236311a623be9d1999fb6dd30eb223cbe16639a9d5502e28879c67aa41915a0b100ce8248360ec73dd2d822087d1c19a8aabc5262a475d8553a35c90dd217
    70812ce8a801517e9e58658fccbc7aa9747ff30475df7c65840424163ee2040d
    sharder
    sharder
    7171
    sharder

  b0mnode0_keys.txt: |
    255452b9f49ebb8c8b8fcec9f0bd8a4284e540be1286bd562578e7e59765e41a7aada04c9e2ad3e28f79aacb0f1be66715535a87983843fea81f23d8011e728b
    8a3f56841f7fd5feb058cae8d16ab87e1c682fd53cfc1204a80c5d0ceb15f509
    miner-0
    miner-0
    7071
    miner

  b0mnode1_keys.txt: |
    17dd0eb10c2567899e34f368e1da2744c9cd3beb2e3babaa4e3350d950bad91dd4ed03d751d49e1f6c1b25c6ec8d1cc8db4ca2da72683d2958fb23843cc6a480
    6587a27fbc132345dec9cd70305cd106c29613908e39a8a41cb085d4a026140e
    miner-1
    miner-1
    7071
    miner1

  b0mnode2_keys.txt: |
    6059d12b7796b101e61c38bb994ebb830280870af2e4ee6d7d7810d110b8e50a61ede17bb5bf31b7208c19f3b9ca6f525ba8e17c14b9f7c8347e84c8375b8d1f
    815933fd14a1e014aa5d8c98de0e342152df3e31396500b37607a62fd9577122
    miner-2
    miner-2
    7071
    miner2

  b0mnode3_keys.txt: |
    aa182e7f1aa1cfcb6cad1e2cbf707db43dbc0afe3437d7d6c657e79cca732122f02a8106891a78b3ebaa2a37ebd148b7ef48f5c0b1b3311094b7f15a1bd7de12
    1e6ec0f5740fe2d01556ba3898e04e3a8e2bde2dc2aeba45dfb9f5e35d28e615
    miner-3
    miner-3
    7071
    miner3

  initial_state.yaml: |
    initialStates:
      - id: 31810bd1258ae95955fb40c7ef72498a556d3587121376d9059119d280f34929
        tokens: 10000000000

      - id: 585732eb076d07455fbebcf3388856b6fd00449a25c47c0f72d961c7c4e7e7c2
        tokens: 10000000000

      - id: 8877e3da19b4cb51e59b4646ec7c0cf4849bc7b860257d69ddbf753b9a981e1b
        tokens: 10000000000

      - id: bfa64c67f49bceec8be618b1b6f558bdbaf9c100fd95d55601fa2190a4e548d8
        tokens: 10000000000

      - id: 57b416fcda1cf82b8a7e1fc3a47c68a94e617be873b5383ea2606bda757d3ce4
        tokens: 10000000000

      # - id: b098d2d56b087ee910f3ee2d2df173630566babb69f0be0e2e9a0c98d63f0b0b
        # tokens: 10000000000

      - id: 6dba10422e368813802877a85039d3985d96760ed844092319743fb3a76712d3
        tokens: 20000000000000000

  magicBlock.json: |
    {
      "hash": "5337382eb82ccec9d6e61030d97091ff798582e28f1a33af254af6cbed78441d",
      "previous_hash": "",
      "magic_block_number": 1,
      "starting_round": 0,
      "miners": {
        "type": 0,
        "nodes": {
          "31810bd1258ae95955fb40c7ef72498a556d3587121376d9059119d280f34929": {
            "id": "31810bd1258ae95955fb40c7ef72498a556d3587121376d9059119d280f34929",
            "version": "",
            "creation_date": 0,
            "public_key": "255452b9f49ebb8c8b8fcec9f0bd8a4284e540be1286bd562578e7e59765e41a7aada04c9e2ad3e28f79aacb0f1be66715535a87983843fea81f23d8011e728b",
            "n2n_host": "RELEASE-NAME-0chain-miner-0",
            "host": "v1.load.testnet-0chain.net",
            "port": 7071,
            "path": "miner0",
            "type": 0,
            "description": "localhost.m0",
            "set_index": 0,
            "status": 0,
            "info": {
              "build_tag": "6941f9204b93b0dc67f3e2a6af359b62a6dd0f76",
              "state_missing_nodes": -1,
              "miners_median_network_time": 0,
              "avg_block_txns": 0
            }
          },
          "585732eb076d07455fbebcf3388856b6fd00449a25c47c0f72d961c7c4e7e7c2": {
            "id": "585732eb076d07455fbebcf3388856b6fd00449a25c47c0f72d961c7c4e7e7c2",
            "version": "",
            "creation_date": 0,
            "public_key": "17dd0eb10c2567899e34f368e1da2744c9cd3beb2e3babaa4e3350d950bad91dd4ed03d751d49e1f6c1b25c6ec8d1cc8db4ca2da72683d2958fb23843cc6a480",
            "n2n_host": "RELEASE-NAME-0chain-miner-1",
            "host": "v1.load.testnet-0chain.net",
            "port": 7071,
            "path": "miner1",
            "type": 0,
            "description": "localhost.m1",
            "set_index": 1,
            "status": 0,
            "info": {
              "build_tag": "",
              "state_missing_nodes": 0,
              "miners_median_network_time": 0,
              "avg_block_txns": 0
            }
          },
          "8877e3da19b4cb51e59b4646ec7c0cf4849bc7b860257d69ddbf753b9a981e1b": {
            "id": "8877e3da19b4cb51e59b4646ec7c0cf4849bc7b860257d69ddbf753b9a981e1b",
            "version": "",
            "creation_date": 0,
            "public_key": "6059d12b7796b101e61c38bb994ebb830280870af2e4ee6d7d7810d110b8e50a61ede17bb5bf31b7208c19f3b9ca6f525ba8e17c14b9f7c8347e84c8375b8d1f",
            "n2n_host": "RELEASE-NAME-0chain-miner-2",
            "host": "v1.load.testnet-0chain.net",
            "port": 7071,
            "path": "miner2",
            "type": 0,
            "description": "localhost.m3",
            "set_index": 2,
            "status": 0,
            "info": {
              "build_tag": "",
              "state_missing_nodes": 0,
              "miners_median_network_time": 0,
              "avg_block_txns": 0
            }
          },
          "bfa64c67f49bceec8be618b1b6f558bdbaf9c100fd95d55601fa2190a4e548d8": {
            "id": "bfa64c67f49bceec8be618b1b6f558bdbaf9c100fd95d55601fa2190a4e548d8",
            "version": "",
            "creation_date": 0,
            "public_key": "aa182e7f1aa1cfcb6cad1e2cbf707db43dbc0afe3437d7d6c657e79cca732122f02a8106891a78b3ebaa2a37ebd148b7ef48f5c0b1b3311094b7f15a1bd7de12",
            "n2n_host": "RELEASE-NAME-0chain-miner-3",
            "host": "v1.load.testnet-0chain.net",
            "port": 7071,
            "path": "miner3",
            "type": 0,
            "description": "localhost.m2",
            "set_index": 3,
            "status": 0,
            "info": {
              "build_tag": "",
              "state_missing_nodes": 0,
              "miners_median_network_time": 0,
              "avg_block_txns": 0
            }
          }
        }
      },
      "sharders": {
        "type": 1,
        "nodes": {
          "57b416fcda1cf82b8a7e1fc3a47c68a94e617be873b5383ea2606bda757d3ce4": {
            "id": "57b416fcda1cf82b8a7e1fc3a47c68a94e617be873b5383ea2606bda757d3ce4",
            "version": "",
            "creation_date": 0,
            "public_key": "b9b236311a623be9d1999fb6dd30eb223cbe16639a9d5502e28879c67aa41915a0b100ce8248360ec73dd2d822087d1c19a8aabc5262a475d8553a35c90dd217",
            "n2n_host": "RELEASE-NAME-0chain-sharder-0",
            "host": "v1.load.testnet-0chain.net",
            "port": 7171,
            "type": 1,
            "path": "sharder0",
            "description": "localhost.s0",
            "set_index": 0,
            "status": 0,
            "info": {
              "build_tag": "",
              "state_missing_nodes": 0,
              "miners_median_network_time": 0,
              "avg_block_txns": 0
            }
          }
        }
      },
      "share_or_signs": {
        "shares": {
          "31810bd1258ae95955fb40c7ef72498a556d3587121376d9059119d280f34929": {
            "id": "31810bd1258ae95955fb40c7ef72498a556d3587121376d9059119d280f34929",
            "share_or_sign": {
              "31810bd1258ae95955fb40c7ef72498a556d3587121376d9059119d280f34929": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "5b011241c42ebeff252eaee67e2044785016307be59b8c3eafe39bfbe5da6b3",
                "sign": "5f5fb823ef8afd27507cfa1d799802e88ca3f3929df1152bab2d20f64d396400"
              },
              "585732eb076d07455fbebcf3388856b6fd00449a25c47c0f72d961c7c4e7e7c2": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "136ffec1171c1335fd54fa70a9eca6e47b750a352b18b55d1af7849043a924f9",
                "sign": "56a0734e6d20db92685105a91c970330f5b318af27a11f0d14b2086d5fee551c"
              },
              "8877e3da19b4cb51e59b4646ec7c0cf4849bc7b860257d69ddbf753b9a981e1b": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "1f393e2c0f4cebc911600fd97503cf53c7135a315224f69cb6b991ac85d125be",
                "sign": "db5136f65ac6fe28774b5a7a1845106d4048dd5da5319be0afae0390ec3ab807"
              },
              "bfa64c67f49bceec8be618b1b6f558bdbaf9c100fd95d55601fa2190a4e548d8": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "218bbf22ba9f6e43c894173c88ad6d9caba466fd4f4e6ee04133ac0285ba2055",
                "sign": "8625116a8fbf620c626791cdba56c1314269d4060d3e34d6663783429880eb17"
              }
            }
          },
          "585732eb076d07455fbebcf3388856b6fd00449a25c47c0f72d961c7c4e7e7c2": {
            "id": "585732eb076d07455fbebcf3388856b6fd00449a25c47c0f72d961c7c4e7e7c2",
            "share_or_sign": {
              "31810bd1258ae95955fb40c7ef72498a556d3587121376d9059119d280f34929": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "1b5cdb6a7c11f963e6c0ebf8709d263205c4742ecb6d14d484cf071411cc17d8",
                "sign": "686ab956814e71a4b111152b01e8c6c137b6de2f6080adde388e739f197bec0f"
              },
              "585732eb076d07455fbebcf3388856b6fd00449a25c47c0f72d961c7c4e7e7c2": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "9cac21e17d4029f5b29282d1005f7f66919fd68c1d3b5c9fb5a05265b5913ce",
                "sign": "7854f0c16c071693001db9152c9fb1c0f86ba46da46f59ffb7a7ba0efad95807"
              },
              "8877e3da19b4cb51e59b4646ec7c0cf4849bc7b860257d69ddbf753b9a981e1b": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "219c52eb501bd309deb07a79407f6962b524e1dc153ea8a56863e3118792bacc",
                "sign": "93dd12f7a62ae4b017d54fddc0c66b43e45b4ded7755cf28b2166e3da85b43a3"
              },
              "bfa64c67f49bceec8be618b1b6f558bdbaf9c100fd95d55601fa2190a4e548d8": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "6aa27dc4bc3a19ab0e13804f0bf83a09e93e5ee32d8a48fd6003f4cac243bdf",
                "sign": "f631d16478ddf9da193e83b9765b60846fa5c258b87bd39e792fb41a36f30d10"
              }
            }
          },
          "8877e3da19b4cb51e59b4646ec7c0cf4849bc7b860257d69ddbf753b9a981e1b": {
            "id": "8877e3da19b4cb51e59b4646ec7c0cf4849bc7b860257d69ddbf753b9a981e1b",
            "share_or_sign": {
              "31810bd1258ae95955fb40c7ef72498a556d3587121376d9059119d280f34929": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "138f8f74f06db34e656e6749c33c6d329de3f1b48de85e0182ba709b19264041",
                "sign": "c9908e2c453b5b1704819bb7f3a8053cad07a2bacc7c0e23a11e17554826ae04"
              },
              "585732eb076d07455fbebcf3388856b6fd00449a25c47c0f72d961c7c4e7e7c2": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "222defdc5e244fbb72f7394b7076a03bf8a916c55229ab2555b586f53f15f902",
                "sign": "8470f7e391f96854bb9bc129dc58a1a0159f8f122597092a8d9b0d7ae4ad7893"
              },
              "8877e3da19b4cb51e59b4646ec7c0cf4849bc7b860257d69ddbf753b9a981e1b": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "7e467fa3bf2b5891b6989932504ff356be94805ccfc62fb0fa1d141b3712415",
                "sign": "d87a2f299d01a7d41bc28bc70bdfe747cb88aec455d8eed277f944f9a6a39690"
              },
              "bfa64c67f49bceec8be618b1b6f558bdbaf9c100fd95d55601fa2190a4e548d8": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "198bc6b7abdf2983def2fdb5cd3e34cdd0e2868ae315d06b56f364dcdc5fe23b",
                "sign": "c8f0ee08500b8db781717aba19c4147117f5407dd039a42041412b863c1ff88b"
              }
            }
          },
          "bfa64c67f49bceec8be618b1b6f558bdbaf9c100fd95d55601fa2190a4e548d8": {
            "id": "bfa64c67f49bceec8be618b1b6f558bdbaf9c100fd95d55601fa2190a4e548d8",
            "share_or_sign": {
              "31810bd1258ae95955fb40c7ef72498a556d3587121376d9059119d280f34929": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "12f3e492e7657f2fba3df2a43c48e07bf6da54cff472d90ff5a27b18c72301cd",
                "sign": "503629d6931b60f19688b5d3b75b90bfc1186118be366d9f732e2285737d520a"
              },
              "585732eb076d07455fbebcf3388856b6fd00449a25c47c0f72d961c7c4e7e7c2": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "18811eb4976c0ca2e4dc3f957753b95d5eee753f20f7e44b6d7e4cccbad332d9",
                "sign": "454a5724885c266501c1c720168b8521b598bc25c8a1994a92b12a89ef00be04"
              },
              "8877e3da19b4cb51e59b4646ec7c0cf4849bc7b860257d69ddbf753b9a981e1b": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "ef45d31fed8bd6e669cc4ba7793362700da2a3bcec49c903e7d97bcb59832da",
                "sign": "b2428c72e327933c9d0ffa5f52042f3619c14e0951530b42639fae41ce845216"
              },
              "bfa64c67f49bceec8be618b1b6f558bdbaf9c100fd95d55601fa2190a4e548d8": {
                "id": "",
                "message": "0chain is AWESOME!!!",
                "share": "c2430abfed391d3f6d72a3d0c76ebe43329b5b79268111bf8c955f513c2d83a",
                "sign": "8e968038e88d797a94bb2d7f663b3f61f13fd2d2e90865ff56dfdf276beeac84"
              }
            }
          }
        }
      },
      "mpks": {
        "Mpks": {
          "31810bd1258ae95955fb40c7ef72498a556d3587121376d9059119d280f34929": {
            "ID": "31810bd1258ae95955fb40c7ef72498a556d3587121376d9059119d280f34929",
            "Mpk": [
              "69cfa0a5c67f25c7be89453956ef20b204f4592cdb82f5e049124ead7339d413a8bf30475dbe87f3c762c4e4d0d31bbae37d047d4a309ea8b5369adbcc6a961d",
              "4f8a41be41f96e895baa62d2e6edbc03071d387e1de64ff37313a8445478e2047d4da4d76f8b9a74006a4ed5e5f4e327ad800188eae0b4ec54ec83fc11833481",
              "356049c4c57955c90a48a25b045d915cc53dde4f5c45115c0ba7c1ba038c641c69a9ad97c7fa3594387fba350996c2931dc746fce8cef0867f68c75da76ddc9d"
            ]
          },
          "585732eb076d07455fbebcf3388856b6fd00449a25c47c0f72d961c7c4e7e7c2": {
            "ID": "585732eb076d07455fbebcf3388856b6fd00449a25c47c0f72d961c7c4e7e7c2",
            "Mpk": [
              "de80c857a945c48d6405d0a6318c1eeb72461bc04d5b87cdd1dbeed6d895741d90a4f434c5eef85f8e138eb4c89c64275dd33d640e5aa95e5e54fef46a294f85",
              "91c64ba152c7ee68656e0e9cf58d3933ba599272450fa4eb203ee251c247842138d7ce12e420f68768995f408d279facfa2a816c59ed425609d72dbdace68a1a",
              "28f250eb834db6d5ae76ed1891b3b805ae195e084c3a2836e8ec1489f498e201850b99d051bbb2ba5318044632a37a663a463db26fce7a3d1e7ef3a455e1d292"
            ]
          },
          "8877e3da19b4cb51e59b4646ec7c0cf4849bc7b860257d69ddbf753b9a981e1b": {
            "ID": "8877e3da19b4cb51e59b4646ec7c0cf4849bc7b860257d69ddbf753b9a981e1b",
            "Mpk": [
              "f810b9f8e21c242ea03bd825b060e5440294d188038efb16fba5b27ed9d66704203bc91b108f11cbc67dda758f114f21f13b2f1902d36d199adda01332edc798",
              "8edded83fa40681a78489b04aed6625547531f3ae7ecd6aaa8d101e74668ec19a2a1cff3cb3a90216066f58d6164246e9729878d0514f466025f0e5ee8ccf481",
              "982ec7ec4f1c21a5b8a3778ec4c52bc8e59bd179b1a04f9c1f01b1be4e6a601e0dd3e41493d24ad041642e3cbda268657d7ff1f3ec1626b2fb2e66434bea019f"
            ]
          },
          "bfa64c67f49bceec8be618b1b6f558bdbaf9c100fd95d55601fa2190a4e548d8": {
            "ID": "bfa64c67f49bceec8be618b1b6f558bdbaf9c100fd95d55601fa2190a4e548d8",
            "Mpk": [
              "acba9ed6b23de84f194640309cd8675583324ac4dacdf8f848bcea70c8d9c51b7b11df25177e4a25448b87c8e0d0b879d07caebc10b71eeab2b5d68c98c895a3",
              "4d913ab974a64e3d3f7f358b7f2d1d47501d85cb6b222404bfc70fa595b21d0131204223951f238c3ca873a2bcd2ef22935cea96044adca4754674306543fb8a",
              "4cd398f67e55deac4e74c5b9df0473da6b31af51b453fe5a56a5a89058ac7705d936f0469d2363281357377866482d3e594b73c1f88d959c2e3a9529c7d75f8d"
            ]
          }
        }
      },
      "t": 3,
      "n": 4
    }
---
# Source: 0chain/templates/redis-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: RELEASE-NAME-0chain-redis
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm-redis
data:
  state.redis.conf: |+
    # Redis configuration file example.
    #
    # Note that in order to read the configuration file, Redis must be
    # started with the file path as first argument:
    #
    # ./redis-server /path/to/redis.conf

    # Note on units: when memory size is needed, it is possible to specify
    # it in the usual form of 1k 5GB 4M and so forth:
    #
    # 1k => 1000 bytes
    # 1kb => 1024 bytes
    # 1m => 1000000 bytes
    # 1mb => 1024*1024 bytes
    # 1g => 1000000000 bytes
    # 1gb => 1024*1024*1024 bytes
    #
    # units are case insensitive so 1GB 1Gb 1gB are all the same.

    ################################## INCLUDES ###################################

    # Include one or more other config files here.  This is useful if you
    # have a standard template that goes to all Redis servers but also need
    # to customize a few per-server settings.  Include files can include
    # other files, so use this wisely.
    #
    # Notice option "include" won't be rewritten by command "CONFIG REWRITE"
    # from admin or Redis Sentinel. Since Redis always uses the last processed
    # line as value of a configuration directive, you'd better put includes
    # at the beginning of this file to avoid overwriting config change at runtime.
    #
    # If instead you are interested in using includes to override configuration
    # options, it is better to use include as the last line.
    #
    # include /path/to/local.conf
    # include /path/to/other.conf

    ################################## MODULES #####################################

    # Load modules at startup. If the server is not able to load modules
    # it will abort. It is possible to use multiple loadmodule directives.
    #
    # loadmodule /path/to/my_module.so
    # loadmodule /path/to/other_module.so

    ################################## NETWORK #####################################

    # By default, if no "bind" configuration directive is specified, Redis listens
    # for connections from all the network interfaces available on the server.
    # It is possible to listen to just one or multiple selected interfaces using
    # the "bind" configuration directive, followed by one or more IP addresses.
    #
    # Examples:
    #
    # bind 192.168.1.100 10.0.0.1
    # bind 127.0.0.1 ::1
    #
    # ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the
    # internet, binding to all the interfaces is dangerous and will expose the
    # instance to everybody on the internet. So by default we uncomment the
    # following bind directive, that will force Redis to listen only into
    # the IPv4 lookback interface address (this means Redis will be able to
    # accept connections only from clients running into the same computer it
    # is running).
    #
    # IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES
    # JUST COMMENT THE FOLLOWING LINE.
    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    # bind 0.0.0.0

    # Protected mode is a layer of security protection, in order to avoid that
    # Redis instances left open on the internet are accessed and exploited.
    #
    # When protected mode is on and if:
    #
    # 1) The server is not binding explicitly to a set of addresses using the
    #    "bind" directive.
    # 2) No password is configured.
    #
    # The server only accepts connections from clients connecting from the
    # IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain
    # sockets.
    #
    # By default protected mode is enabled. You should disable it only if
    # you are sure you want clients from other hosts to connect to Redis
    # even if no authentication is configured, nor a specific set of interfaces
    # are explicitly listed using the "bind" directive.
    protected-mode no

    # Accept connections on the specified port, default is 6379 (IANA #815344).
    # If port 0 is specified Redis will not listen on a TCP socket.
    port 6379

    # TCP listen() backlog.
    #
    # In high requests-per-second environments you need an high backlog in order
    # to avoid slow clients connections issues. Note that the Linux kernel
    # will silently truncate it to the value of /proc/sys/net/core/somaxconn so
    # make sure to raise both the value of somaxconn and tcp_max_syn_backlog
    # in order to get the desired effect.
    tcp-backlog 511

    # Unix socket.
    #
    # Specify the path for the Unix socket that will be used to listen for
    # incoming connections. There is no default, so Redis will not listen
    # on a unix socket when not specified.
    #
    # unixsocket /tmp/redis.sock
    # unixsocketperm 700

    # Close the connection after a client is idle for N seconds (0 to disable)
    timeout 0

    # TCP keepalive.
    #
    # If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence
    # of communication. This is useful for two reasons:
    #
    # 1) Detect dead peers.
    # 2) Take the connection alive from the point of view of network
    #    equipment in the middle.
    #
    # On Linux, the specified value (in seconds) is the period used to send ACKs.
    # Note that to close the connection the double of the time is needed.
    # On other kernels the period depends on the kernel configuration.
    #
    # A reasonable value for this option is 300 seconds, which is the new
    # Redis default starting with Redis 3.2.1.
    tcp-keepalive 300

    ################################# GENERAL #####################################

    # By default Redis does not run as a daemon. Use 'yes' if you need it.
    # Note that Redis will write a pid file in /usr/local/homebrew/var/run/redis.pid when daemonized.
    daemonize no

    # If you run Redis from upstart or systemd, Redis can interact with your
    # supervision tree. Options:
    #   supervised no      - no supervision interaction
    #   supervised upstart - signal upstart by putting Redis into SIGSTOP mode
    #   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET
    #   supervised auto    - detect upstart or systemd method based on
    #                        UPSTART_JOB or NOTIFY_SOCKET environment variables
    # Note: these supervision methods only signal "process is ready."
    #       They do not enable continuous liveness pings back to your supervisor.
    supervised no

    # If a pid file is specified, Redis writes it where specified at startup
    # and removes it at exit.
    #
    # When the server runs non daemonized, no pid file is created if none is
    # specified in the configuration. When the server is daemonized, the pid file
    # is used even if not specified, defaulting to "/usr/local/homebrew/var/run/redis.pid".
    #
    # Creating a pid file is best effort: if Redis is not able to create it
    # nothing bad happens, the server will start and run normally.
    pidfile /var/run/redis_6379.pid

    # Specify the server verbosity level.
    # This can be one of:
    # debug (a lot of information, useful for development/testing)
    # verbose (many rarely useful info, but not a mess like the debug level)
    # notice (moderately verbose, what you want in production probably)
    # warning (only very important / critical messages are logged)
    loglevel notice

    # Specify the log file name. Also the empty string can be used to force
    # Redis to log on the standard output. Note that if you use standard
    # output for logging but daemonize, logs will be sent to /dev/null
    logfile ""

    # To enable logging to the system logger, just set 'syslog-enabled' to yes,
    # and optionally update the other syslog parameters to suit your needs.
    # syslog-enabled no

    # Specify the syslog identity.
    # syslog-ident redis

    # Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.
    # syslog-facility local0

    # Set the number of databases. The default database is DB 0, you can select
    # a different one on a per-connection basis using SELECT <dbid> where
    # dbid is a number between 0 and 'databases'-1
    databases 16

    # By default Redis shows an ASCII art logo only when started to log to the
    # standard output and if the standard output is a TTY. Basically this means
    # that normally a logo is displayed only in interactive sessions.
    #
    # However it is possible to force the pre-4.0 behavior and always show a
    # ASCII art logo in startup logs by setting the following option to yes.
    always-show-logo yes

    ################################ SNAPSHOTTING  ################################
    #
    # Save the DB on disk:
    #
    #   save <seconds> <changes>
    #
    #   Will save the DB if both the given number of seconds and the given
    #   number of write operations against the DB occurred.
    #
    #   In the example below the behaviour will be to save:
    #   after 900 sec (15 min) if at least 1 key changed
    #   after 300 sec (5 min) if at least 10 keys changed
    #   after 60 sec if at least 10000 keys changed
    #
    #   Note: you can disable saving completely by commenting out all "save" lines.
    #
    #   It is also possible to remove all the previously configured save
    #   points by adding a save directive with a single empty string argument
    #   like in the following example:
    #
    #   save ""

    save 900 1
    save 300 10
    save 60 10000

    # By default Redis will stop accepting writes if RDB snapshots are enabled
    # (at least one save point) and the latest background save failed.
    # This will make the user aware (in a hard way) that data is not persisting
    # on disk properly, otherwise chances are that no one will notice and some
    # disaster will happen.
    #
    # If the background saving process will start working again Redis will
    # automatically allow writes again.
    #
    # However if you have setup your proper monitoring of the Redis server
    # and persistence, you may want to disable this feature so that Redis will
    # continue to work as usual even if there are problems with disk,
    # permissions, and so forth.
    stop-writes-on-bgsave-error yes

    # Compress string objects using LZF when dump .rdb databases?
    # For default that's set to 'yes' as it's almost always a win.
    # If you want to save some CPU in the saving child set it to 'no' but
    # the dataset will likely be bigger if you have compressible values or keys.
    rdbcompression yes

    # Since version 5 of RDB a CRC64 checksum is placed at the end of the file.
    # This makes the format more resistant to corruption but there is a performance
    # hit to pay (around 10%) when saving and loading RDB files, so you can disable it
    # for maximum performances.
    #
    # RDB files created with checksum disabled have a checksum of zero that will
    # tell the loading code to skip the check.
    rdbchecksum yes

    # The filename where to dump the DB
    dbfilename state.dump.rdb

    # The working directory.
    #
    # The DB will be written inside this directory, with the filename specified
    # above using the 'dbfilename' configuration directive.
    #
    # The Append Only File will also be created inside this directory.
    #
    # Note that you must specify a directory here, not a file name.
    dir /0chain/data/redis/state

    ################################# REPLICATION #################################

    # Master-Slave replication. Use slaveof to make a Redis instance a copy of
    # another Redis server. A few things to understand ASAP about Redis replication.
    #
    # 1) Redis replication is asynchronous, but you can configure a master to
    #    stop accepting writes if it appears to be not connected with at least
    #    a given number of slaves.
    # 2) Redis slaves are able to perform a partial resynchronization with the
    #    master if the replication link is lost for a relatively small amount of
    #    time. You may want to configure the replication backlog size (see the next
    #    sections of this file) with a sensible value depending on your needs.
    # 3) Replication is automatic and does not need user intervention. After a
    #    network partition slaves automatically try to reconnect to masters
    #    and resynchronize with them.
    #
    # slaveof <masterip> <masterport>

    # If the master is password protected (using the "requirepass" configuration
    # directive below) it is possible to tell the slave to authenticate before
    # starting the replication synchronization process, otherwise the master will
    # refuse the slave request.
    #
    # masterauth <master-password>

    # When a slave loses its connection with the master, or when the replication
    # is still in progress, the slave can act in two different ways:
    #
    # 1) if slave-serve-stale-data is set to 'yes' (the default) the slave will
    #    still reply to client requests, possibly with out of date data, or the
    #    data set may just be empty if this is the first synchronization.
    #
    # 2) if slave-serve-stale-data is set to 'no' the slave will reply with
    #    an error "SYNC with master in progress" to all the kind of commands
    #    but to INFO and SLAVEOF.
    #
    slave-serve-stale-data yes

    # You can configure a slave instance to accept writes or not. Writing against
    # a slave instance may be useful to store some ephemeral data (because data
    # written on a slave will be easily deleted after resync with the master) but
    # may also cause problems if clients are writing to it because of a
    # misconfiguration.
    #
    # Since Redis 2.6 by default slaves are read-only.
    #
    # Note: read only slaves are not designed to be exposed to untrusted clients
    # on the internet. It's just a protection layer against misuse of the instance.
    # Still a read only slave exports by default all the administrative commands
    # such as CONFIG, DEBUG, and so forth. To a limited extent you can improve
    # security of read only slaves using 'rename-command' to shadow all the
    # administrative / dangerous commands.
    slave-read-only yes

    # Replication SYNC strategy: disk or socket.
    #
    # -------------------------------------------------------
    # WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY
    # -------------------------------------------------------
    #
    # New slaves and reconnecting slaves that are not able to continue the replication
    # process just receiving differences, need to do what is called a "full
    # synchronization". An RDB file is transmitted from the master to the slaves.
    # The transmission can happen in two different ways:
    #
    # 1) Disk-backed: The Redis master creates a new process that writes the RDB
    #                 file on disk. Later the file is transferred by the parent
    #                 process to the slaves incrementally.
    # 2) Diskless: The Redis master creates a new process that directly writes the
    #              RDB file to slave sockets, without touching the disk at all.
    #
    # With disk-backed replication, while the RDB file is generated, more slaves
    # can be queued and served with the RDB file as soon as the current child producing
    # the RDB file finishes its work. With diskless replication instead once
    # the transfer starts, new slaves arriving will be queued and a new transfer
    # will start when the current one terminates.
    #
    # When diskless replication is used, the master waits a configurable amount of
    # time (in seconds) before starting the transfer in the hope that multiple slaves
    # will arrive and the transfer can be parallelized.
    #
    # With slow disks and fast (large bandwidth) networks, diskless replication
    # works better.
    repl-diskless-sync no

    # When diskless replication is enabled, it is possible to configure the delay
    # the server waits in order to spawn the child that transfers the RDB via socket
    # to the slaves.
    #
    # This is important since once the transfer starts, it is not possible to serve
    # new slaves arriving, that will be queued for the next RDB transfer, so the server
    # waits a delay in order to let more slaves arrive.
    #
    # The delay is specified in seconds, and by default is 5 seconds. To disable
    # it entirely just set it to 0 seconds and the transfer will start ASAP.
    repl-diskless-sync-delay 5

    # Slaves send PINGs to server in a predefined interval. It's possible to change
    # this interval with the repl_ping_slave_period option. The default value is 10
    # seconds.
    #
    # repl-ping-slave-period 10

    # The following option sets the replication timeout for:
    #
    # 1) Bulk transfer I/O during SYNC, from the point of view of slave.
    # 2) Master timeout from the point of view of slaves (data, pings).
    # 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).
    #
    # It is important to make sure that this value is greater than the value
    # specified for repl-ping-slave-period otherwise a timeout will be detected
    # every time there is low traffic between the master and the slave.
    #
    # repl-timeout 60

    # Disable TCP_NODELAY on the slave socket after SYNC?
    #
    # If you select "yes" Redis will use a smaller number of TCP packets and
    # less bandwidth to send data to slaves. But this can add a delay for
    # the data to appear on the slave side, up to 40 milliseconds with
    # Linux kernels using a default configuration.
    #
    # If you select "no" the delay for data to appear on the slave side will
    # be reduced but more bandwidth will be used for replication.
    #
    # By default we optimize for low latency, but in very high traffic conditions
    # or when the master and slaves are many hops away, turning this to "yes" may
    # be a good idea.
    repl-disable-tcp-nodelay no

    # Set the replication backlog size. The backlog is a buffer that accumulates
    # slave data when slaves are disconnected for some time, so that when a slave
    # wants to reconnect again, often a full resync is not needed, but a partial
    # resync is enough, just passing the portion of data the slave missed while
    # disconnected.
    #
    # The bigger the replication backlog, the longer the time the slave can be
    # disconnected and later be able to perform a partial resynchronization.
    #
    # The backlog is only allocated once there is at least a slave connected.
    #
    # repl-backlog-size 1mb

    # After a master has no longer connected slaves for some time, the backlog
    # will be freed. The following option configures the amount of seconds that
    # need to elapse, starting from the time the last slave disconnected, for
    # the backlog buffer to be freed.
    #
    # Note that slaves never free the backlog for timeout, since they may be
    # promoted to masters later, and should be able to correctly "partially
    # resynchronize" with the slaves: hence they should always accumulate backlog.
    #
    # A value of 0 means to never release the backlog.
    #
    # repl-backlog-ttl 3600

    # The slave priority is an integer number published by Redis in the INFO output.
    # It is used by Redis Sentinel in order to select a slave to promote into a
    # master if the master is no longer working correctly.
    #
    # A slave with a low priority number is considered better for promotion, so
    # for instance if there are three slaves with priority 10, 100, 25 Sentinel will
    # pick the one with priority 10, that is the lowest.
    #
    # However a special priority of 0 marks the slave as not able to perform the
    # role of master, so a slave with priority of 0 will never be selected by
    # Redis Sentinel for promotion.
    #
    # By default the priority is 100.
    slave-priority 100

    # It is possible for a master to stop accepting writes if there are less than
    # N slaves connected, having a lag less or equal than M seconds.
    #
    # The N slaves need to be in "online" state.
    #
    # The lag in seconds, that must be <= the specified value, is calculated from
    # the last ping received from the slave, that is usually sent every second.
    #
    # This option does not GUARANTEE that N replicas will accept the write, but
    # will limit the window of exposure for lost writes in case not enough slaves
    # are available, to the specified number of seconds.
    #
    # For example to require at least 3 slaves with a lag <= 10 seconds use:
    #
    # min-slaves-to-write 3
    # min-slaves-max-lag 10
    #
    # Setting one or the other to 0 disables the feature.
    #
    # By default min-slaves-to-write is set to 0 (feature disabled) and
    # min-slaves-max-lag is set to 10.

    # A Redis master is able to list the address and port of the attached
    # slaves in different ways. For example the "INFO replication" section
    # offers this information, which is used, among other tools, by
    # Redis Sentinel in order to discover slave instances.
    # Another place where this info is available is in the output of the
    # "ROLE" command of a master.
    #
    # The listed IP and address normally reported by a slave is obtained
    # in the following way:
    #
    #   IP: The address is auto detected by checking the peer address
    #   of the socket used by the slave to connect with the master.
    #
    #   Port: The port is communicated by the slave during the replication
    #   handshake, and is normally the port that the slave is using to
    #   list for connections.
    #
    # However when port forwarding or Network Address Translation (NAT) is
    # used, the slave may be actually reachable via different IP and port
    # pairs. The following two options can be used by a slave in order to
    # report to its master a specific set of IP and port, so that both INFO
    # and ROLE will report those values.
    #
    # There is no need to use both the options if you need to override just
    # the port or the IP address.
    #
    # slave-announce-ip 5.5.5.5
    # slave-announce-port 1234

    ################################## SECURITY ###################################

    # Require clients to issue AUTH <PASSWORD> before processing any other
    # commands.  This might be useful in environments in which you do not trust
    # others with access to the host running redis-server.
    #
    # This should stay commented out for backward compatibility and because most
    # people do not need auth (e.g. they run their own servers).
    #
    # Warning: since Redis is pretty fast an outside user can try up to
    # 150k passwords per second against a good box. This means that you should
    # use a very strong password otherwise it will be very easy to break.
    #
    # requirepass foobared

    # Command renaming.
    #
    # It is possible to change the name of dangerous commands in a shared
    # environment. For instance the CONFIG command may be renamed into something
    # hard to guess so that it will still be available for internal-use tools
    # but not available for general clients.
    #
    # Example:
    #
    # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52
    #
    # It is also possible to completely kill a command by renaming it into
    # an empty string:
    #
    # rename-command CONFIG ""
    #
    # Please note that changing the name of commands that are logged into the
    # AOF file or transmitted to slaves may cause problems.

    ################################### CLIENTS ####################################

    # Set the max number of connected clients at the same time. By default
    # this limit is set to 10000 clients, however if the Redis server is not
    # able to configure the process file limit to allow for the specified limit
    # the max number of allowed clients is set to the current file limit
    # minus 32 (as Redis reserves a few file descriptors for internal uses).
    #
    # Once the limit is reached Redis will close all the new connections sending
    # an error 'max number of clients reached'.
    #
    # maxclients 10000

    ############################## MEMORY MANAGEMENT ################################

    # Set a memory usage limit to the specified amount of bytes.
    # When the memory limit is reached Redis will try to remove keys
    # according to the eviction policy selected (see maxmemory-policy).
    #
    # If Redis can't remove keys according to the policy, or if the policy is
    # set to 'noeviction', Redis will start to reply with errors to commands
    # that would use more memory, like SET, LPUSH, and so on, and will continue
    # to reply to read-only commands like GET.
    #
    # This option is usually useful when using Redis as an LRU or LFU cache, or to
    # set a hard memory limit for an instance (using the 'noeviction' policy).
    #
    # WARNING: If you have slaves attached to an instance with maxmemory on,
    # the size of the output buffers needed to feed the slaves are subtracted
    # from the used memory count, so that network problems / resyncs will
    # not trigger a loop where keys are evicted, and in turn the output
    # buffer of slaves is full with DELs of keys evicted triggering the deletion
    # of more keys, and so forth until the database is completely emptied.
    #
    # In short... if you have slaves attached it is suggested that you set a lower
    # limit for maxmemory so that there is some free RAM on the system for slave
    # output buffers (but this is not needed if the policy is 'noeviction').
    #
    # maxmemory <bytes>

    # MAXMEMORY POLICY: how Redis will select what to remove when maxmemory
    # is reached. You can select among five behaviors:
    #
    # volatile-lru -> Evict using approximated LRU among the keys with an expire set.
    # allkeys-lru -> Evict any key using approximated LRU.
    # volatile-lfu -> Evict using approximated LFU among the keys with an expire set.
    # allkeys-lfu -> Evict any key using approximated LFU.
    # volatile-random -> Remove a random key among the ones with an expire set.
    # allkeys-random -> Remove a random key, any key.
    # volatile-ttl -> Remove the key with the nearest expire time (minor TTL)
    # noeviction -> Don't evict anything, just return an error on write operations.
    #
    # LRU means Least Recently Used
    # LFU means Least Frequently Used
    #
    # Both LRU, LFU and volatile-ttl are implemented using approximated
    # randomized algorithms.
    #
    # Note: with any of the above policies, Redis will return an error on write
    #       operations, when there are no suitable keys for eviction.
    #
    #       At the date of writing these commands are: set setnx setex append
    #       incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd
    #       sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby
    #       zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby
    #       getset mset msetnx exec sort
    #
    # The default is:
    #
    # maxmemory-policy noeviction

    # LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated
    # algorithms (in order to save memory), so you can tune it for speed or
    # accuracy. For default Redis will check five keys and pick the one that was
    # used less recently, you can change the sample size using the following
    # configuration directive.
    #
    # The default of 5 produces good enough results. 10 Approximates very closely
    # true LRU but costs more CPU. 3 is faster but not very accurate.
    #
    # maxmemory-samples 5

    ############################# LAZY FREEING ####################################

    # Redis has two primitives to delete keys. One is called DEL and is a blocking
    # deletion of the object. It means that the server stops processing new commands
    # in order to reclaim all the memory associated with an object in a synchronous
    # way. If the key deleted is associated with a small object, the time needed
    # in order to execute the DEL command is very small and comparable to most other
    # O(1) or O(log_N) commands in Redis. However if the key is associated with an
    # aggregated value containing millions of elements, the server can block for
    # a long time (even seconds) in order to complete the operation.
    #
    # For the above reasons Redis also offers non blocking deletion primitives
    # such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and
    # FLUSHDB commands, in order to reclaim memory in background. Those commands
    # are executed in constant time. Another thread will incrementally free the
    # object in the background as fast as possible.
    #
    # DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.
    # It's up to the design of the application to understand when it is a good
    # idea to use one or the other. However the Redis server sometimes has to
    # delete keys or flush the whole database as a side effect of other operations.
    # Specifically Redis deletes objects independently of a user call in the
    # following scenarios:
    #
    # 1) On eviction, because of the maxmemory and maxmemory policy configurations,
    #    in order to make room for new data, without going over the specified
    #    memory limit.
    # 2) Because of expire: when a key with an associated time to live (see the
    #    EXPIRE command) must be deleted from memory.
    # 3) Because of a side effect of a command that stores data on a key that may
    #    already exist. For example the RENAME command may delete the old key
    #    content when it is replaced with another one. Similarly SUNIONSTORE
    #    or SORT with STORE option may delete existing keys. The SET command
    #    itself removes any old content of the specified key in order to replace
    #    it with the specified string.
    # 4) During replication, when a slave performs a full resynchronization with
    #    its master, the content of the whole database is removed in order to
    #    load the RDB file just transfered.
    #
    # In all the above cases the default is to delete objects in a blocking way,
    # like if DEL was called. However you can configure each case specifically
    # in order to instead release memory in a non-blocking way like if UNLINK
    # was called, using the following configuration directives:

    lazyfree-lazy-eviction no
    lazyfree-lazy-expire no
    lazyfree-lazy-server-del no
    slave-lazy-flush no

    ############################## APPEND ONLY MODE ###############################

    # By default Redis asynchronously dumps the dataset on disk. This mode is
    # good enough in many applications, but an issue with the Redis process or
    # a power outage may result into a few minutes of writes lost (depending on
    # the configured save points).
    #
    # The Append Only File is an alternative persistence mode that provides
    # much better durability. For instance using the default data fsync policy
    # (see later in the config file) Redis can lose just one second of writes in a
    # dramatic event like a server power outage, or a single write if something
    # wrong with the Redis process itself happens, but the operating system is
    # still running correctly.
    #
    # AOF and RDB persistence can be enabled at the same time without problems.
    # If the AOF is enabled on startup Redis will load the AOF, that is the file
    # with the better durability guarantees.
    #
    # Please check http://redis.io/topics/persistence for more information.

    appendonly no

    # The name of the append only file (default: "appendonly.aof")

    appendfilename "appendonly.aof"

    # The fsync() call tells the Operating System to actually write data on disk
    # instead of waiting for more data in the output buffer. Some OS will really flush
    # data on disk, some other OS will just try to do it ASAP.
    #
    # Redis supports three different modes:
    #
    # no: don't fsync, just let the OS flush the data when it wants. Faster.
    # always: fsync after every write to the append only log. Slow, Safest.
    # everysec: fsync only one time every second. Compromise.
    #
    # The default is "everysec", as that's usually the right compromise between
    # speed and data safety. It's up to you to understand if you can relax this to
    # "no" that will let the operating system flush the output buffer when
    # it wants, for better performances (but if you can live with the idea of
    # some data loss consider the default persistence mode that's snapshotting),
    # or on the contrary, use "always" that's very slow but a bit safer than
    # everysec.
    #
    # More details please check the following article:
    # http://antirez.com/post/redis-persistence-demystified.html
    #
    # If unsure, use "everysec".

    # appendfsync always
    appendfsync everysec
    # appendfsync no

    # When the AOF fsync policy is set to always or everysec, and a background
    # saving process (a background save or AOF log background rewriting) is
    # performing a lot of I/O against the disk, in some Linux configurations
    # Redis may block too long on the fsync() call. Note that there is no fix for
    # this currently, as even performing fsync in a different thread will block
    # our synchronous write(2) call.
    #
    # In order to mitigate this problem it's possible to use the following option
    # that will prevent fsync() from being called in the main process while a
    # BGSAVE or BGREWRITEAOF is in progress.
    #
    # This means that while another child is saving, the durability of Redis is
    # the same as "appendfsync none". In practical terms, this means that it is
    # possible to lose up to 30 seconds of log in the worst scenario (with the
    # default Linux settings).
    #
    # If you have latency problems turn this to "yes". Otherwise leave it as
    # "no" that is the safest pick from the point of view of durability.

    no-appendfsync-on-rewrite no

    # Automatic rewrite of the append only file.
    # Redis is able to automatically rewrite the log file implicitly calling
    # BGREWRITEAOF when the AOF log size grows by the specified percentage.
    #
    # This is how it works: Redis remembers the size of the AOF file after the
    # latest rewrite (if no rewrite has happened since the restart, the size of
    # the AOF at startup is used).
    #
    # This base size is compared to the current size. If the current size is
    # bigger than the specified percentage, the rewrite is triggered. Also
    # you need to specify a minimal size for the AOF file to be rewritten, this
    # is useful to avoid rewriting the AOF file even if the percentage increase
    # is reached but it is still pretty small.
    #
    # Specify a percentage of zero in order to disable the automatic AOF
    # rewrite feature.

    auto-aof-rewrite-percentage 100
    auto-aof-rewrite-min-size 64mb

    # An AOF file may be found to be truncated at the end during the Redis
    # startup process, when the AOF data gets loaded back into memory.
    # This may happen when the system where Redis is running
    # crashes, especially when an ext4 filesystem is mounted without the
    # data=ordered option (however this can't happen when Redis itself
    # crashes or aborts but the operating system still works correctly).
    #
    # Redis can either exit with an error when this happens, or load as much
    # data as possible (the default now) and start if the AOF file is found
    # to be truncated at the end. The following option controls this behavior.
    #
    # If aof-load-truncated is set to yes, a truncated AOF file is loaded and
    # the Redis server starts emitting a log to inform the user of the event.
    # Otherwise if the option is set to no, the server aborts with an error
    # and refuses to start. When the option is set to no, the user requires
    # to fix the AOF file using the "redis-check-aof" utility before to restart
    # the server.
    #
    # Note that if the AOF file will be found to be corrupted in the middle
    # the server will still exit with an error. This option only applies when
    # Redis will try to read more data from the AOF file but not enough bytes
    # will be found.
    aof-load-truncated yes

    # When rewriting the AOF file, Redis is able to use an RDB preamble in the
    # AOF file for faster rewrites and recoveries. When this option is turned
    # on the rewritten AOF file is composed of two different stanzas:
    #
    #   [RDB file][AOF tail]
    #
    # When loading Redis recognizes that the AOF file starts with the "REDIS"
    # string and loads the prefixed RDB file, and continues loading the AOF
    # tail.
    #
    # This is currently turned off by default in order to avoid the surprise
    # of a format change, but will at some point be used as the default.
    aof-use-rdb-preamble no

    ################################ LUA SCRIPTING  ###############################

    # Max execution time of a Lua script in milliseconds.
    #
    # If the maximum execution time is reached Redis will log that a script is
    # still in execution after the maximum allowed time and will start to
    # reply to queries with an error.
    #
    # When a long running script exceeds the maximum execution time only the
    # SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be
    # used to stop a script that did not yet called write commands. The second
    # is the only way to shut down the server in the case a write command was
    # already issued by the script but the user doesn't want to wait for the natural
    # termination of the script.
    #
    # Set it to 0 or a negative value for unlimited execution without warnings.
    lua-time-limit 5000

    ################################ REDIS CLUSTER  ###############################
    #
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
    # WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however
    # in order to mark it as "mature" we need to wait for a non trivial percentage
    # of users to deploy it in production.
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
    #
    # Normal Redis instances can't be part of a Redis Cluster; only nodes that are
    # started as cluster nodes can. In order to start a Redis instance as a
    # cluster node enable the cluster support uncommenting the following:
    #
    # cluster-enabled yes

    # Every cluster node has a cluster configuration file. This file is not
    # intended to be edited by hand. It is created and updated by Redis nodes.
    # Every Redis Cluster node requires a different cluster configuration file.
    # Make sure that instances running in the same system do not have
    # overlapping cluster configuration file names.
    #
    # cluster-config-file nodes-6379.conf

    # Cluster node timeout is the amount of milliseconds a node must be unreachable
    # for it to be considered in failure state.
    # Most other internal time limits are multiple of the node timeout.
    #
    # cluster-node-timeout 15000

    # A slave of a failing master will avoid to start a failover if its data
    # looks too old.
    #
    # There is no simple way for a slave to actually have an exact measure of
    # its "data age", so the following two checks are performed:
    #
    # 1) If there are multiple slaves able to failover, they exchange messages
    #    in order to try to give an advantage to the slave with the best
    #    replication offset (more data from the master processed).
    #    Slaves will try to get their rank by offset, and apply to the start
    #    of the failover a delay proportional to their rank.
    #
    # 2) Every single slave computes the time of the last interaction with
    #    its master. This can be the last ping or command received (if the master
    #    is still in the "connected" state), or the time that elapsed since the
    #    disconnection with the master (if the replication link is currently down).
    #    If the last interaction is too old, the slave will not try to failover
    #    at all.
    #
    # The point "2" can be tuned by user. Specifically a slave will not perform
    # the failover if, since the last interaction with the master, the time
    # elapsed is greater than:
    #
    #   (node-timeout * slave-validity-factor) + repl-ping-slave-period
    #
    # So for example if node-timeout is 30 seconds, and the slave-validity-factor
    # is 10, and assuming a default repl-ping-slave-period of 10 seconds, the
    # slave will not try to failover if it was not able to talk with the master
    # for longer than 310 seconds.
    #
    # A large slave-validity-factor may allow slaves with too old data to failover
    # a master, while a too small value may prevent the cluster from being able to
    # elect a slave at all.
    #
    # For maximum availability, it is possible to set the slave-validity-factor
    # to a value of 0, which means, that slaves will always try to failover the
    # master regardless of the last time they interacted with the master.
    # (However they'll always try to apply a delay proportional to their
    # offset rank).
    #
    # Zero is the only value able to guarantee that when all the partitions heal
    # the cluster will always be able to continue.
    #
    # cluster-slave-validity-factor 10

    # Cluster slaves are able to migrate to orphaned masters, that are masters
    # that are left without working slaves. This improves the cluster ability
    # to resist to failures as otherwise an orphaned master can't be failed over
    # in case of failure if it has no working slaves.
    #
    # Slaves migrate to orphaned masters only if there are still at least a
    # given number of other working slaves for their old master. This number
    # is the "migration barrier". A migration barrier of 1 means that a slave
    # will migrate only if there is at least 1 other working slave for its master
    # and so forth. It usually reflects the number of slaves you want for every
    # master in your cluster.
    #
    # Default is 1 (slaves migrate only if their masters remain with at least
    # one slave). To disable migration just set it to a very large value.
    # A value of 0 can be set but is useful only for debugging and dangerous
    # in production.
    #
    # cluster-migration-barrier 1

    # By default Redis Cluster nodes stop accepting queries if they detect there
    # is at least an hash slot uncovered (no available node is serving it).
    # This way if the cluster is partially down (for example a range of hash slots
    # are no longer covered) all the cluster becomes, eventually, unavailable.
    # It automatically returns available as soon as all the slots are covered again.
    #
    # However sometimes you want the subset of the cluster which is working,
    # to continue to accept queries for the part of the key space that is still
    # covered. In order to do so, just set the cluster-require-full-coverage
    # option to no.
    #
    # cluster-require-full-coverage yes

    # This option, when set to yes, prevents slaves from trying to failover its
    # master during master failures. However the master can still perform a
    # manual failover, if forced to do so.
    #
    # This is useful in different scenarios, especially in the case of multiple
    # data center operations, where we want one side to never be promoted if not
    # in the case of a total DC failure.
    #
    # cluster-slave-no-failover no

    # In order to setup your cluster make sure to read the documentation
    # available at http://redis.io web site.

    ########################## CLUSTER DOCKER/NAT support  ########################

    # In certain deployments, Redis Cluster nodes address discovery fails, because
    # addresses are NAT-ted or because ports are forwarded (the typical case is
    # Docker and other containers).
    #
    # In order to make Redis Cluster working in such environments, a static
    # configuration where each node knows its public address is needed. The
    # following two options are used for this scope, and are:
    #
    # * cluster-announce-ip
    # * cluster-announce-port
    # * cluster-announce-bus-port
    #
    # Each instruct the node about its address, client port, and cluster message
    # bus port. The information is then published in the header of the bus packets
    # so that other nodes will be able to correctly map the address of the node
    # publishing the information.
    #
    # If the above options are not used, the normal Redis Cluster auto-detection
    # will be used instead.
    #
    # Note that when remapped, the bus port may not be at the fixed offset of
    # clients port + 10000, so you can specify any port and bus-port depending
    # on how they get remapped. If the bus-port is not set, a fixed offset of
    # 10000 will be used as usually.
    #
    # Example:
    #
    # cluster-announce-ip 10.1.1.5
    # cluster-announce-port 6379
    # cluster-announce-bus-port 6380

    ################################## SLOW LOG ###################################

    # The Redis Slow Log is a system to log queries that exceeded a specified
    # execution time. The execution time does not include the I/O operations
    # like talking with the client, sending the reply and so forth,
    # but just the time needed to actually execute the command (this is the only
    # stage of command execution where the thread is blocked and can not serve
    # other requests in the meantime).
    #
    # You can configure the slow log with two parameters: one tells Redis
    # what is the execution time, in microseconds, to exceed in order for the
    # command to get logged, and the other parameter is the length of the
    # slow log. When a new command is logged the oldest one is removed from the
    # queue of logged commands.

    # The following time is expressed in microseconds, so 1000000 is equivalent
    # to one second. Note that a negative number disables the slow log, while
    # a value of zero forces the logging of every command.
    slowlog-log-slower-than 10000

    # There is no limit to this length. Just be aware that it will consume memory.
    # You can reclaim memory used by the slow log with SLOWLOG RESET.
    slowlog-max-len 128

    ################################ LATENCY MONITOR ##############################

    # The Redis latency monitoring subsystem samples different operations
    # at runtime in order to collect data related to possible sources of
    # latency of a Redis instance.
    #
    # Via the LATENCY command this information is available to the user that can
    # print graphs and obtain reports.
    #
    # The system only logs operations that were performed in a time equal or
    # greater than the amount of milliseconds specified via the
    # latency-monitor-threshold configuration directive. When its value is set
    # to zero, the latency monitor is turned off.
    #
    # By default latency monitoring is disabled since it is mostly not needed
    # if you don't have latency issues, and collecting data has a performance
    # impact, that while very small, can be measured under big load. Latency
    # monitoring can easily be enabled at runtime using the command
    # "CONFIG SET latency-monitor-threshold <milliseconds>" if needed.
    latency-monitor-threshold 0

    ############################# EVENT NOTIFICATION ##############################

    # Redis can notify Pub/Sub clients about events happening in the key space.
    # This feature is documented at http://redis.io/topics/notifications
    #
    # For instance if keyspace events notification is enabled, and a client
    # performs a DEL operation on key "foo" stored in the Database 0, two
    # messages will be published via Pub/Sub:
    #
    # PUBLISH __keyspace@0__:foo del
    # PUBLISH __keyevent@0__:del foo
    #
    # It is possible to select the events that Redis will notify among a set
    # of classes. Every class is identified by a single character:
    #
    #  K     Keyspace events, published with __keyspace@<db>__ prefix.
    #  E     Keyevent events, published with __keyevent@<db>__ prefix.
    #  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...
    #  $     String commands
    #  l     List commands
    #  s     Set commands
    #  h     Hash commands
    #  z     Sorted set commands
    #  x     Expired events (events generated every time a key expires)
    #  e     Evicted events (events generated when a key is evicted for maxmemory)
    #  A     Alias for g$lshzxe, so that the "AKE" string means all the events.
    #
    #  The "notify-keyspace-events" takes as argument a string that is composed
    #  of zero or multiple characters. The empty string means that notifications
    #  are disabled.
    #
    #  Example: to enable list and generic events, from the point of view of the
    #           event name, use:
    #
    #  notify-keyspace-events Elg
    #
    #  Example 2: to get the stream of the expired keys subscribing to channel
    #             name __keyevent@0__:expired use:
    #
    #  notify-keyspace-events Ex
    #
    #  By default all notifications are disabled because most users don't need
    #  this feature and the feature has some overhead. Note that if you don't
    #  specify at least one of K or E, no events will be delivered.
    notify-keyspace-events ""

    ############################### ADVANCED CONFIG ###############################

    # Hashes are encoded using a memory efficient data structure when they have a
    # small number of entries, and the biggest entry does not exceed a given
    # threshold. These thresholds can be configured using the following directives.
    hash-max-ziplist-entries 512
    hash-max-ziplist-value 64

    # Lists are also encoded in a special way to save a lot of space.
    # The number of entries allowed per internal list node can be specified
    # as a fixed maximum size or a maximum number of elements.
    # For a fixed maximum size, use -5 through -1, meaning:
    # -5: max size: 64 Kb  <-- not recommended for normal workloads
    # -4: max size: 32 Kb  <-- not recommended
    # -3: max size: 16 Kb  <-- probably not recommended
    # -2: max size: 8 Kb   <-- good
    # -1: max size: 4 Kb   <-- good
    # Positive numbers mean store up to _exactly_ that number of elements
    # per list node.
    # The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),
    # but if your use case is unique, adjust the settings as necessary.
    list-max-ziplist-size -2

    # Lists may also be compressed.
    # Compress depth is the number of quicklist ziplist nodes from *each* side of
    # the list to *exclude* from compression.  The head and tail of the list
    # are always uncompressed for fast push/pop operations.  Settings are:
    # 0: disable all list compression
    # 1: depth 1 means "don't start compressing until after 1 node into the list,
    #    going from either the head or tail"
    #    So: [head]->node->node->...->node->[tail]
    #    [head], [tail] will always be uncompressed; inner nodes will compress.
    # 2: [head]->[next]->node->node->...->node->[prev]->[tail]
    #    2 here means: don't compress head or head->next or tail->prev or tail,
    #    but compress all nodes between them.
    # 3: [head]->[next]->[next]->node->node->...->node->[prev]->[prev]->[tail]
    # etc.
    list-compress-depth 0

    # Sets have a special encoding in just one case: when a set is composed
    # of just strings that happen to be integers in radix 10 in the range
    # of 64 bit signed integers.
    # The following configuration setting sets the limit in the size of the
    # set in order to use this special memory saving encoding.
    set-max-intset-entries 512

    # Similarly to hashes and lists, sorted sets are also specially encoded in
    # order to save a lot of space. This encoding is only used when the length and
    # elements of a sorted set are below the following limits:
    zset-max-ziplist-entries 128
    zset-max-ziplist-value 64

    # HyperLogLog sparse representation bytes limit. The limit includes the
    # 16 bytes header. When an HyperLogLog using the sparse representation crosses
    # this limit, it is converted into the dense representation.
    #
    # A value greater than 16000 is totally useless, since at that point the
    # dense representation is more memory efficient.
    #
    # The suggested value is ~ 3000 in order to have the benefits of
    # the space efficient encoding without slowing down too much PFADD,
    # which is O(N) with the sparse encoding. The value can be raised to
    # ~ 10000 when CPU is not a concern, but space is, and the data set is
    # composed of many HyperLogLogs with cardinality in the 0 - 15000 range.
    hll-sparse-max-bytes 3000

    # Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in
    # order to help rehashing the main Redis hash table (the one mapping top-level
    # keys to values). The hash table implementation Redis uses (see dict.c)
    # performs a lazy rehashing: the more operation you run into a hash table
    # that is rehashing, the more rehashing "steps" are performed, so if the
    # server is idle the rehashing is never complete and some more memory is used
    # by the hash table.
    #
    # The default is to use this millisecond 10 times every second in order to
    # actively rehash the main dictionaries, freeing memory when possible.
    #
    # If unsure:
    # use "activerehashing no" if you have hard latency requirements and it is
    # not a good thing in your environment that Redis can reply from time to time
    # to queries with 2 milliseconds delay.
    #
    # use "activerehashing yes" if you don't have such hard requirements but
    # want to free memory asap when possible.
    activerehashing yes

    # The client output buffer limits can be used to force disconnection of clients
    # that are not reading data from the server fast enough for some reason (a
    # common reason is that a Pub/Sub client can't consume messages as fast as the
    # publisher can produce them).
    #
    # The limit can be set differently for the three different classes of clients:
    #
    # normal -> normal clients including MONITOR clients
    # slave  -> slave clients
    # pubsub -> clients subscribed to at least one pubsub channel or pattern
    #
    # The syntax of every client-output-buffer-limit directive is the following:
    #
    # client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds>
    #
    # A client is immediately disconnected once the hard limit is reached, or if
    # the soft limit is reached and remains reached for the specified number of
    # seconds (continuously).
    # So for instance if the hard limit is 32 megabytes and the soft limit is
    # 16 megabytes / 10 seconds, the client will get disconnected immediately
    # if the size of the output buffers reach 32 megabytes, but will also get
    # disconnected if the client reaches 16 megabytes and continuously overcomes
    # the limit for 10 seconds.
    #
    # By default normal clients are not limited because they don't receive data
    # without asking (in a push way), but just after a request, so only
    # asynchronous clients may create a scenario where data is requested faster
    # than it can read.
    #
    # Instead there is a default limit for pubsub and slave clients, since
    # subscribers and slaves receive data in a push fashion.
    #
    # Both the hard or the soft limit can be disabled by setting them to zero.
    client-output-buffer-limit normal 0 0 0
    client-output-buffer-limit slave 256mb 64mb 60
    client-output-buffer-limit pubsub 32mb 8mb 60

    # Client query buffers accumulate new commands. They are limited to a fixed
    # amount by default in order to avoid that a protocol desynchronization (for
    # instance due to a bug in the client) will lead to unbound memory usage in
    # the query buffer. However you can configure it here if you have very special
    # needs, such us huge multi/exec requests or alike.
    #
    # client-query-buffer-limit 1gb

    # In the Redis protocol, bulk requests, that are, elements representing single
    # strings, are normally limited ot 512 mb. However you can change this limit
    # here.
    #
    # proto-max-bulk-len 512mb

    # Redis calls an internal function to perform many background tasks, like
    # closing connections of clients in timeout, purging expired keys that are
    # never requested, and so forth.
    #
    # Not all tasks are performed with the same frequency, but Redis checks for
    # tasks to perform according to the specified "hz" value.
    #
    # By default "hz" is set to 10. Raising the value will use more CPU when
    # Redis is idle, but at the same time will make Redis more responsive when
    # there are many keys expiring at the same time, and timeouts may be
    # handled with more precision.
    #
    # The range is between 1 and 500, however a value over 100 is usually not
    # a good idea. Most users should use the default of 10 and raise this up to
    # 100 only in environments where very low latency is required.
    hz 10

    # When a child rewrites the AOF file, if the following option is enabled
    # the file will be fsync-ed every 32 MB of data generated. This is useful
    # in order to commit the file to the disk more incrementally and avoid
    # big latency spikes.
    aof-rewrite-incremental-fsync yes

    # Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good
    # idea to start with the default settings and only change them after investigating
    # how to improve the performances and how the keys LFU change over time, which
    # is possible to inspect via the OBJECT FREQ command.
    #
    # There are two tunable parameters in the Redis LFU implementation: the
    # counter logarithm factor and the counter decay time. It is important to
    # understand what the two parameters mean before changing them.
    #
    # The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis
    # uses a probabilistic increment with logarithmic behavior. Given the value
    # of the old counter, when a key is accessed, the counter is incremented in
    # this way:
    #
    # 1. A random number R between 0 and 1 is extracted.
    # 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).
    # 3. The counter is incremented only if R < P.
    #
    # The default lfu-log-factor is 10. This is a table of how the frequency
    # counter changes with a different number of accesses with different
    # logarithmic factors:
    #
    # +--------+------------+------------+------------+------------+------------+
    # | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   |
    # +--------+------------+------------+------------+------------+------------+
    # | 0      | 104        | 255        | 255        | 255        | 255        |
    # +--------+------------+------------+------------+------------+------------+
    # | 1      | 18         | 49         | 255        | 255        | 255        |
    # +--------+------------+------------+------------+------------+------------+
    # | 10     | 10         | 18         | 142        | 255        | 255        |
    # +--------+------------+------------+------------+------------+------------+
    # | 100    | 8          | 11         | 49         | 143        | 255        |
    # +--------+------------+------------+------------+------------+------------+
    #
    # NOTE: The above table was obtained by running the following commands:
    #
    #   redis-benchmark -n 1000000 incr foo
    #   redis-cli object freq foo
    #
    # NOTE 2: The counter initial value is 5 in order to give new objects a chance
    # to accumulate hits.
    #
    # The counter decay time is the time, in minutes, that must elapse in order
    # for the key counter to be divided by two (or decremented if it has a value
    # less <= 10).
    #
    # The default value for the lfu-decay-time is 1. A Special value of 0 means to
    # decay the counter every time it happens to be scanned.
    #
    # lfu-log-factor 10
    # lfu-decay-time 1

    ########################### ACTIVE DEFRAGMENTATION #######################
    #
    # WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested
    # even in production and manually tested by multiple engineers for some
    # time.
    #
    # What is active defragmentation?
    # -------------------------------
    #
    # Active (online) defragmentation allows a Redis server to compact the
    # spaces left between small allocations and deallocations of data in memory,
    # thus allowing to reclaim back memory.
    #
    # Fragmentation is a natural process that happens with every allocator (but
    # less so with Jemalloc, fortunately) and certain workloads. Normally a server
    # restart is needed in order to lower the fragmentation, or at least to flush
    # away all the data and create it again. However thanks to this feature
    # implemented by Oran Agra for Redis 4.0 this process can happen at runtime
    # in an "hot" way, while the server is running.
    #
    # Basically when the fragmentation is over a certain level (see the
    # configuration options below) Redis will start to create new copies of the
    # values in contiguous memory regions by exploiting certain specific Jemalloc
    # features (in order to understand if an allocation is causing fragmentation
    # and to allocate it in a better place), and at the same time, will release the
    # old copies of the data. This process, repeated incrementally for all the keys
    # will cause the fragmentation to drop back to normal values.
    #
    # Important things to understand:
    #
    # 1. This feature is disabled by default, and only works if you compiled Redis
    #    to use the copy of Jemalloc we ship with the source code of Redis.
    #    This is the default with Linux builds.
    #
    # 2. You never need to enable this feature if you don't have fragmentation
    #    issues.
    #
    # 3. Once you experience fragmentation, you can enable this feature when
    #    needed with the command "CONFIG SET activedefrag yes".
    #
    # The configuration parameters are able to fine tune the behavior of the
    # defragmentation process. If you are not sure about what they mean it is
    # a good idea to leave the defaults untouched.

    # Enabled active defragmentation
    # activedefrag yes

    # Minimum amount of fragmentation waste to start active defrag
    # active-defrag-ignore-bytes 100mb

    # Minimum percentage of fragmentation to start active defrag
    # active-defrag-threshold-lower 10

    # Maximum percentage of fragmentation at which we use maximum effort
    # active-defrag-threshold-upper 100

    # Minimal effort for defrag in CPU percentage
    # active-defrag-cycle-min 25

    # Maximal effort for defrag in CPU percentage
    # active-defrag-cycle-max 75

  transactions.redis.conf: |+
    # Redis configuration file example.
    #
    # Note that in order to read the configuration file, Redis must be
    # started with the file path as first argument:
    #
    # ./redis-server /path/to/redis.conf

    # Note on units: when memory size is needed, it is possible to specify
    # it in the usual form of 1k 5GB 4M and so forth:
    #
    # 1k => 1000 bytes
    # 1kb => 1024 bytes
    # 1m => 1000000 bytes
    # 1mb => 1024*1024 bytes
    # 1g => 1000000000 bytes
    # 1gb => 1024*1024*1024 bytes
    #
    # units are case insensitive so 1GB 1Gb 1gB are all the same.

    ################################## INCLUDES ###################################

    # Include one or more other config files here.  This is useful if you
    # have a standard template that goes to all Redis servers but also need
    # to customize a few per-server settings.  Include files can include
    # other files, so use this wisely.
    #
    # Notice option "include" won't be rewritten by command "CONFIG REWRITE"
    # from admin or Redis Sentinel. Since Redis always uses the last processed
    # line as value of a configuration directive, you'd better put includes
    # at the beginning of this file to avoid overwriting config change at runtime.
    #
    # If instead you are interested in using includes to override configuration
    # options, it is better to use include as the last line.
    #
    # include /path/to/local.conf
    # include /path/to/other.conf

    ################################## MODULES #####################################

    # Load modules at startup. If the server is not able to load modules
    # it will abort. It is possible to use multiple loadmodule directives.
    #
    # loadmodule /path/to/my_module.so
    # loadmodule /path/to/other_module.so

    ################################## NETWORK #####################################

    # By default, if no "bind" configuration directive is specified, Redis listens
    # for connections from all the network interfaces available on the server.
    # It is possible to listen to just one or multiple selected interfaces using
    # the "bind" configuration directive, followed by one or more IP addresses.
    #
    # Examples:
    #
    # bind 192.168.1.100 10.0.0.1
    # bind 127.0.0.1 ::1
    #
    # ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the
    # internet, binding to all the interfaces is dangerous and will expose the
    # instance to everybody on the internet. So by default we uncomment the
    # following bind directive, that will force Redis to listen only into
    # the IPv4 lookback interface address (this means Redis will be able to
    # accept connections only from clients running into the same computer it
    # is running).
    #
    # IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES
    # JUST COMMENT THE FOLLOWING LINE.
    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    # bind 0.0.0.0

    # Protected mode is a layer of security protection, in order to avoid that
    # Redis instances left open on the internet are accessed and exploited.
    #
    # When protected mode is on and if:
    #
    # 1) The server is not binding explicitly to a set of addresses using the
    #    "bind" directive.
    # 2) No password is configured.
    #
    # The server only accepts connections from clients connecting from the
    # IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain
    # sockets.
    #
    # By default protected mode is enabled. You should disable it only if
    # you are sure you want clients from other hosts to connect to Redis
    # even if no authentication is configured, nor a specific set of interfaces
    # are explicitly listed using the "bind" directive.
    protected-mode no

    # Accept connections on the specified port, default is 6379 (IANA #815344).
    # If port 0 is specified Redis will not listen on a TCP socket.
    port 6479

    # TCP listen() backlog.
    #
    # In high requests-per-second environments you need an high backlog in order
    # to avoid slow clients connections issues. Note that the Linux kernel
    # will silently truncate it to the value of /proc/sys/net/core/somaxconn so
    # make sure to raise both the value of somaxconn and tcp_max_syn_backlog
    # in order to get the desired effect.
    tcp-backlog 511

    # Unix socket.
    #
    # Specify the path for the Unix socket that will be used to listen for
    # incoming connections. There is no default, so Redis will not listen
    # on a unix socket when not specified.
    #
    # unixsocket /tmp/redis.sock
    # unixsocketperm 700

    # Close the connection after a client is idle for N seconds (0 to disable)
    timeout 0

    # TCP keepalive.
    #
    # If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence
    # of communication. This is useful for two reasons:
    #
    # 1) Detect dead peers.
    # 2) Take the connection alive from the point of view of network
    #    equipment in the middle.
    #
    # On Linux, the specified value (in seconds) is the period used to send ACKs.
    # Note that to close the connection the double of the time is needed.
    # On other kernels the period depends on the kernel configuration.
    #
    # A reasonable value for this option is 300 seconds, which is the new
    # Redis default starting with Redis 3.2.1.
    tcp-keepalive 300

    ################################# GENERAL #####################################

    # By default Redis does not run as a daemon. Use 'yes' if you need it.
    # Note that Redis will write a pid file in /usr/local/homebrew/var/run/redis.pid when daemonized.
    daemonize no

    # If you run Redis from upstart or systemd, Redis can interact with your
    # supervision tree. Options:
    #   supervised no      - no supervision interaction
    #   supervised upstart - signal upstart by putting Redis into SIGSTOP mode
    #   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET
    #   supervised auto    - detect upstart or systemd method based on
    #                        UPSTART_JOB or NOTIFY_SOCKET environment variables
    # Note: these supervision methods only signal "process is ready."
    #       They do not enable continuous liveness pings back to your supervisor.
    supervised no

    # If a pid file is specified, Redis writes it where specified at startup
    # and removes it at exit.
    #
    # When the server runs non daemonized, no pid file is created if none is
    # specified in the configuration. When the server is daemonized, the pid file
    # is used even if not specified, defaulting to "/usr/local/homebrew/var/run/redis.pid".
    #
    # Creating a pid file is best effort: if Redis is not able to create it
    # nothing bad happens, the server will start and run normally.
    pidfile /var/run/redis_6379.pid

    # Specify the server verbosity level.
    # This can be one of:
    # debug (a lot of information, useful for development/testing)
    # verbose (many rarely useful info, but not a mess like the debug level)
    # notice (moderately verbose, what you want in production probably)
    # warning (only very important / critical messages are logged)
    loglevel notice

    # Specify the log file name. Also the empty string can be used to force
    # Redis to log on the standard output. Note that if you use standard
    # output for logging but daemonize, logs will be sent to /dev/null
    logfile ""

    # To enable logging to the system logger, just set 'syslog-enabled' to yes,
    # and optionally update the other syslog parameters to suit your needs.
    # syslog-enabled no

    # Specify the syslog identity.
    # syslog-ident redis

    # Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.
    # syslog-facility local0

    # Set the number of databases. The default database is DB 0, you can select
    # a different one on a per-connection basis using SELECT <dbid> where
    # dbid is a number between 0 and 'databases'-1
    databases 16

    # By default Redis shows an ASCII art logo only when started to log to the
    # standard output and if the standard output is a TTY. Basically this means
    # that normally a logo is displayed only in interactive sessions.
    #
    # However it is possible to force the pre-4.0 behavior and always show a
    # ASCII art logo in startup logs by setting the following option to yes.
    always-show-logo yes

    ################################ SNAPSHOTTING  ################################
    #
    # Save the DB on disk:
    #
    #   save <seconds> <changes>
    #
    #   Will save the DB if both the given number of seconds and the given
    #   number of write operations against the DB occurred.
    #
    #   In the example below the behaviour will be to save:
    #   after 900 sec (15 min) if at least 1 key changed
    #   after 300 sec (5 min) if at least 10 keys changed
    #   after 60 sec if at least 10000 keys changed
    #
    #   Note: you can disable saving completely by commenting out all "save" lines.
    #
    #   It is also possible to remove all the previously configured save
    #   points by adding a save directive with a single empty string argument
    #   like in the following example:
    #
    #   save ""

    #save 900 1
    #save 300 10
    #save 60 10000
    save ""

    # By default Redis will stop accepting writes if RDB snapshots are enabled
    # (at least one save point) and the latest background save failed.
    # This will make the user aware (in a hard way) that data is not persisting
    # on disk properly, otherwise chances are that no one will notice and some
    # disaster will happen.
    #
    # If the background saving process will start working again Redis will
    # automatically allow writes again.
    #
    # However if you have setup your proper monitoring of the Redis server
    # and persistence, you may want to disable this feature so that Redis will
    # continue to work as usual even if there are problems with disk,
    # permissions, and so forth.
    stop-writes-on-bgsave-error yes

    # Compress string objects using LZF when dump .rdb databases?
    # For default that's set to 'yes' as it's almost always a win.
    # If you want to save some CPU in the saving child set it to 'no' but
    # the dataset will likely be bigger if you have compressible values or keys.
    rdbcompression yes

    # Since version 5 of RDB a CRC64 checksum is placed at the end of the file.
    # This makes the format more resistant to corruption but there is a performance
    # hit to pay (around 10%) when saving and loading RDB files, so you can disable it
    # for maximum performances.
    #
    # RDB files created with checksum disabled have a checksum of zero that will
    # tell the loading code to skip the check.
    rdbchecksum yes

    # The filename where to dump the DB
    dbfilename transactions.dump.rdb

    # The working directory.
    #
    # The DB will be written inside this directory, with the filename specified
    # above using the 'dbfilename' configuration directive.
    #
    # The Append Only File will also be created inside this directory.
    #
    # Note that you must specify a directory here, not a file name.
    dir /0chain/data/redis/transactions

    ################################# REPLICATION #################################

    # Master-Slave replication. Use slaveof to make a Redis instance a copy of
    # another Redis server. A few things to understand ASAP about Redis replication.
    #
    # 1) Redis replication is asynchronous, but you can configure a master to
    #    stop accepting writes if it appears to be not connected with at least
    #    a given number of slaves.
    # 2) Redis slaves are able to perform a partial resynchronization with the
    #    master if the replication link is lost for a relatively small amount of
    #    time. You may want to configure the replication backlog size (see the next
    #    sections of this file) with a sensible value depending on your needs.
    # 3) Replication is automatic and does not need user intervention. After a
    #    network partition slaves automatically try to reconnect to masters
    #    and resynchronize with them.
    #
    # slaveof <masterip> <masterport>

    # If the master is password protected (using the "requirepass" configuration
    # directive below) it is possible to tell the slave to authenticate before
    # starting the replication synchronization process, otherwise the master will
    # refuse the slave request.
    #
    # masterauth <master-password>

    # When a slave loses its connection with the master, or when the replication
    # is still in progress, the slave can act in two different ways:
    #
    # 1) if slave-serve-stale-data is set to 'yes' (the default) the slave will
    #    still reply to client requests, possibly with out of date data, or the
    #    data set may just be empty if this is the first synchronization.
    #
    # 2) if slave-serve-stale-data is set to 'no' the slave will reply with
    #    an error "SYNC with master in progress" to all the kind of commands
    #    but to INFO and SLAVEOF.
    #
    slave-serve-stale-data yes

    # You can configure a slave instance to accept writes or not. Writing against
    # a slave instance may be useful to store some ephemeral data (because data
    # written on a slave will be easily deleted after resync with the master) but
    # may also cause problems if clients are writing to it because of a
    # misconfiguration.
    #
    # Since Redis 2.6 by default slaves are read-only.
    #
    # Note: read only slaves are not designed to be exposed to untrusted clients
    # on the internet. It's just a protection layer against misuse of the instance.
    # Still a read only slave exports by default all the administrative commands
    # such as CONFIG, DEBUG, and so forth. To a limited extent you can improve
    # security of read only slaves using 'rename-command' to shadow all the
    # administrative / dangerous commands.
    slave-read-only yes

    # Replication SYNC strategy: disk or socket.
    #
    # -------------------------------------------------------
    # WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY
    # -------------------------------------------------------
    #
    # New slaves and reconnecting slaves that are not able to continue the replication
    # process just receiving differences, need to do what is called a "full
    # synchronization". An RDB file is transmitted from the master to the slaves.
    # The transmission can happen in two different ways:
    #
    # 1) Disk-backed: The Redis master creates a new process that writes the RDB
    #                 file on disk. Later the file is transferred by the parent
    #                 process to the slaves incrementally.
    # 2) Diskless: The Redis master creates a new process that directly writes the
    #              RDB file to slave sockets, without touching the disk at all.
    #
    # With disk-backed replication, while the RDB file is generated, more slaves
    # can be queued and served with the RDB file as soon as the current child producing
    # the RDB file finishes its work. With diskless replication instead once
    # the transfer starts, new slaves arriving will be queued and a new transfer
    # will start when the current one terminates.
    #
    # When diskless replication is used, the master waits a configurable amount of
    # time (in seconds) before starting the transfer in the hope that multiple slaves
    # will arrive and the transfer can be parallelized.
    #
    # With slow disks and fast (large bandwidth) networks, diskless replication
    # works better.
    repl-diskless-sync no

    # When diskless replication is enabled, it is possible to configure the delay
    # the server waits in order to spawn the child that transfers the RDB via socket
    # to the slaves.
    #
    # This is important since once the transfer starts, it is not possible to serve
    # new slaves arriving, that will be queued for the next RDB transfer, so the server
    # waits a delay in order to let more slaves arrive.
    #
    # The delay is specified in seconds, and by default is 5 seconds. To disable
    # it entirely just set it to 0 seconds and the transfer will start ASAP.
    repl-diskless-sync-delay 5

    # Slaves send PINGs to server in a predefined interval. It's possible to change
    # this interval with the repl_ping_slave_period option. The default value is 10
    # seconds.
    #
    # repl-ping-slave-period 10

    # The following option sets the replication timeout for:
    #
    # 1) Bulk transfer I/O during SYNC, from the point of view of slave.
    # 2) Master timeout from the point of view of slaves (data, pings).
    # 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).
    #
    # It is important to make sure that this value is greater than the value
    # specified for repl-ping-slave-period otherwise a timeout will be detected
    # every time there is low traffic between the master and the slave.
    #
    # repl-timeout 60

    # Disable TCP_NODELAY on the slave socket after SYNC?
    #
    # If you select "yes" Redis will use a smaller number of TCP packets and
    # less bandwidth to send data to slaves. But this can add a delay for
    # the data to appear on the slave side, up to 40 milliseconds with
    # Linux kernels using a default configuration.
    #
    # If you select "no" the delay for data to appear on the slave side will
    # be reduced but more bandwidth will be used for replication.
    #
    # By default we optimize for low latency, but in very high traffic conditions
    # or when the master and slaves are many hops away, turning this to "yes" may
    # be a good idea.
    repl-disable-tcp-nodelay no

    # Set the replication backlog size. The backlog is a buffer that accumulates
    # slave data when slaves are disconnected for some time, so that when a slave
    # wants to reconnect again, often a full resync is not needed, but a partial
    # resync is enough, just passing the portion of data the slave missed while
    # disconnected.
    #
    # The bigger the replication backlog, the longer the time the slave can be
    # disconnected and later be able to perform a partial resynchronization.
    #
    # The backlog is only allocated once there is at least a slave connected.
    #
    # repl-backlog-size 1mb

    # After a master has no longer connected slaves for some time, the backlog
    # will be freed. The following option configures the amount of seconds that
    # need to elapse, starting from the time the last slave disconnected, for
    # the backlog buffer to be freed.
    #
    # Note that slaves never free the backlog for timeout, since they may be
    # promoted to masters later, and should be able to correctly "partially
    # resynchronize" with the slaves: hence they should always accumulate backlog.
    #
    # A value of 0 means to never release the backlog.
    #
    # repl-backlog-ttl 3600

    # The slave priority is an integer number published by Redis in the INFO output.
    # It is used by Redis Sentinel in order to select a slave to promote into a
    # master if the master is no longer working correctly.
    #
    # A slave with a low priority number is considered better for promotion, so
    # for instance if there are three slaves with priority 10, 100, 25 Sentinel will
    # pick the one with priority 10, that is the lowest.
    #
    # However a special priority of 0 marks the slave as not able to perform the
    # role of master, so a slave with priority of 0 will never be selected by
    # Redis Sentinel for promotion.
    #
    # By default the priority is 100.
    slave-priority 100

    # It is possible for a master to stop accepting writes if there are less than
    # N slaves connected, having a lag less or equal than M seconds.
    #
    # The N slaves need to be in "online" state.
    #
    # The lag in seconds, that must be <= the specified value, is calculated from
    # the last ping received from the slave, that is usually sent every second.
    #
    # This option does not GUARANTEE that N replicas will accept the write, but
    # will limit the window of exposure for lost writes in case not enough slaves
    # are available, to the specified number of seconds.
    #
    # For example to require at least 3 slaves with a lag <= 10 seconds use:
    #
    # min-slaves-to-write 3
    # min-slaves-max-lag 10
    #
    # Setting one or the other to 0 disables the feature.
    #
    # By default min-slaves-to-write is set to 0 (feature disabled) and
    # min-slaves-max-lag is set to 10.

    # A Redis master is able to list the address and port of the attached
    # slaves in different ways. For example the "INFO replication" section
    # offers this information, which is used, among other tools, by
    # Redis Sentinel in order to discover slave instances.
    # Another place where this info is available is in the output of the
    # "ROLE" command of a master.
    #
    # The listed IP and address normally reported by a slave is obtained
    # in the following way:
    #
    #   IP: The address is auto detected by checking the peer address
    #   of the socket used by the slave to connect with the master.
    #
    #   Port: The port is communicated by the slave during the replication
    #   handshake, and is normally the port that the slave is using to
    #   list for connections.
    #
    # However when port forwarding or Network Address Translation (NAT) is
    # used, the slave may be actually reachable via different IP and port
    # pairs. The following two options can be used by a slave in order to
    # report to its master a specific set of IP and port, so that both INFO
    # and ROLE will report those values.
    #
    # There is no need to use both the options if you need to override just
    # the port or the IP address.
    #
    # slave-announce-ip 5.5.5.5
    # slave-announce-port 1234

    ################################## SECURITY ###################################

    # Require clients to issue AUTH <PASSWORD> before processing any other
    # commands.  This might be useful in environments in which you do not trust
    # others with access to the host running redis-server.
    #
    # This should stay commented out for backward compatibility and because most
    # people do not need auth (e.g. they run their own servers).
    #
    # Warning: since Redis is pretty fast an outside user can try up to
    # 150k passwords per second against a good box. This means that you should
    # use a very strong password otherwise it will be very easy to break.
    #
    # requirepass foobared

    # Command renaming.
    #
    # It is possible to change the name of dangerous commands in a shared
    # environment. For instance the CONFIG command may be renamed into something
    # hard to guess so that it will still be available for internal-use tools
    # but not available for general clients.
    #
    # Example:
    #
    # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52
    #
    # It is also possible to completely kill a command by renaming it into
    # an empty string:
    #
    # rename-command CONFIG ""
    #
    # Please note that changing the name of commands that are logged into the
    # AOF file or transmitted to slaves may cause problems.

    ################################### CLIENTS ####################################

    # Set the max number of connected clients at the same time. By default
    # this limit is set to 10000 clients, however if the Redis server is not
    # able to configure the process file limit to allow for the specified limit
    # the max number of allowed clients is set to the current file limit
    # minus 32 (as Redis reserves a few file descriptors for internal uses).
    #
    # Once the limit is reached Redis will close all the new connections sending
    # an error 'max number of clients reached'.
    #
    # maxclients 10000

    ############################## MEMORY MANAGEMENT ################################

    # Set a memory usage limit to the specified amount of bytes.
    # When the memory limit is reached Redis will try to remove keys
    # according to the eviction policy selected (see maxmemory-policy).
    #
    # If Redis can't remove keys according to the policy, or if the policy is
    # set to 'noeviction', Redis will start to reply with errors to commands
    # that would use more memory, like SET, LPUSH, and so on, and will continue
    # to reply to read-only commands like GET.
    #
    # This option is usually useful when using Redis as an LRU or LFU cache, or to
    # set a hard memory limit for an instance (using the 'noeviction' policy).
    #
    # WARNING: If you have slaves attached to an instance with maxmemory on,
    # the size of the output buffers needed to feed the slaves are subtracted
    # from the used memory count, so that network problems / resyncs will
    # not trigger a loop where keys are evicted, and in turn the output
    # buffer of slaves is full with DELs of keys evicted triggering the deletion
    # of more keys, and so forth until the database is completely emptied.
    #
    # In short... if you have slaves attached it is suggested that you set a lower
    # limit for maxmemory so that there is some free RAM on the system for slave
    # output buffers (but this is not needed if the policy is 'noeviction').
    #
    # maxmemory <bytes>

    # MAXMEMORY POLICY: how Redis will select what to remove when maxmemory
    # is reached. You can select among five behaviors:
    #
    # volatile-lru -> Evict using approximated LRU among the keys with an expire set.
    # allkeys-lru -> Evict any key using approximated LRU.
    # volatile-lfu -> Evict using approximated LFU among the keys with an expire set.
    # allkeys-lfu -> Evict any key using approximated LFU.
    # volatile-random -> Remove a random key among the ones with an expire set.
    # allkeys-random -> Remove a random key, any key.
    # volatile-ttl -> Remove the key with the nearest expire time (minor TTL)
    # noeviction -> Don't evict anything, just return an error on write operations.
    #
    # LRU means Least Recently Used
    # LFU means Least Frequently Used
    #
    # Both LRU, LFU and volatile-ttl are implemented using approximated
    # randomized algorithms.
    #
    # Note: with any of the above policies, Redis will return an error on write
    #       operations, when there are no suitable keys for eviction.
    #
    #       At the date of writing these commands are: set setnx setex append
    #       incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd
    #       sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby
    #       zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby
    #       getset mset msetnx exec sort
    #
    # The default is:
    #
    # maxmemory-policy noeviction

    # LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated
    # algorithms (in order to save memory), so you can tune it for speed or
    # accuracy. For default Redis will check five keys and pick the one that was
    # used less recently, you can change the sample size using the following
    # configuration directive.
    #
    # The default of 5 produces good enough results. 10 Approximates very closely
    # true LRU but costs more CPU. 3 is faster but not very accurate.
    #
    # maxmemory-samples 5

    ############################# LAZY FREEING ####################################

    # Redis has two primitives to delete keys. One is called DEL and is a blocking
    # deletion of the object. It means that the server stops processing new commands
    # in order to reclaim all the memory associated with an object in a synchronous
    # way. If the key deleted is associated with a small object, the time needed
    # in order to execute the DEL command is very small and comparable to most other
    # O(1) or O(log_N) commands in Redis. However if the key is associated with an
    # aggregated value containing millions of elements, the server can block for
    # a long time (even seconds) in order to complete the operation.
    #
    # For the above reasons Redis also offers non blocking deletion primitives
    # such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and
    # FLUSHDB commands, in order to reclaim memory in background. Those commands
    # are executed in constant time. Another thread will incrementally free the
    # object in the background as fast as possible.
    #
    # DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.
    # It's up to the design of the application to understand when it is a good
    # idea to use one or the other. However the Redis server sometimes has to
    # delete keys or flush the whole database as a side effect of other operations.
    # Specifically Redis deletes objects independently of a user call in the
    # following scenarios:
    #
    # 1) On eviction, because of the maxmemory and maxmemory policy configurations,
    #    in order to make room for new data, without going over the specified
    #    memory limit.
    # 2) Because of expire: when a key with an associated time to live (see the
    #    EXPIRE command) must be deleted from memory.
    # 3) Because of a side effect of a command that stores data on a key that may
    #    already exist. For example the RENAME command may delete the old key
    #    content when it is replaced with another one. Similarly SUNIONSTORE
    #    or SORT with STORE option may delete existing keys. The SET command
    #    itself removes any old content of the specified key in order to replace
    #    it with the specified string.
    # 4) During replication, when a slave performs a full resynchronization with
    #    its master, the content of the whole database is removed in order to
    #    load the RDB file just transfered.
    #
    # In all the above cases the default is to delete objects in a blocking way,
    # like if DEL was called. However you can configure each case specifically
    # in order to instead release memory in a non-blocking way like if UNLINK
    # was called, using the following configuration directives:

    lazyfree-lazy-eviction no
    lazyfree-lazy-expire no
    lazyfree-lazy-server-del no
    slave-lazy-flush no

    ############################## APPEND ONLY MODE ###############################

    # By default Redis asynchronously dumps the dataset on disk. This mode is
    # good enough in many applications, but an issue with the Redis process or
    # a power outage may result into a few minutes of writes lost (depending on
    # the configured save points).
    #
    # The Append Only File is an alternative persistence mode that provides
    # much better durability. For instance using the default data fsync policy
    # (see later in the config file) Redis can lose just one second of writes in a
    # dramatic event like a server power outage, or a single write if something
    # wrong with the Redis process itself happens, but the operating system is
    # still running correctly.
    #
    # AOF and RDB persistence can be enabled at the same time without problems.
    # If the AOF is enabled on startup Redis will load the AOF, that is the file
    # with the better durability guarantees.
    #
    # Please check http://redis.io/topics/persistence for more information.

    appendonly no

    # The name of the append only file (default: "appendonly.aof")

    appendfilename "appendonly.aof"

    # The fsync() call tells the Operating System to actually write data on disk
    # instead of waiting for more data in the output buffer. Some OS will really flush
    # data on disk, some other OS will just try to do it ASAP.
    #
    # Redis supports three different modes:
    #
    # no: don't fsync, just let the OS flush the data when it wants. Faster.
    # always: fsync after every write to the append only log. Slow, Safest.
    # everysec: fsync only one time every second. Compromise.
    #
    # The default is "everysec", as that's usually the right compromise between
    # speed and data safety. It's up to you to understand if you can relax this to
    # "no" that will let the operating system flush the output buffer when
    # it wants, for better performances (but if you can live with the idea of
    # some data loss consider the default persistence mode that's snapshotting),
    # or on the contrary, use "always" that's very slow but a bit safer than
    # everysec.
    #
    # More details please check the following article:
    # http://antirez.com/post/redis-persistence-demystified.html
    #
    # If unsure, use "everysec".

    # appendfsync always
    appendfsync everysec
    # appendfsync no

    # When the AOF fsync policy is set to always or everysec, and a background
    # saving process (a background save or AOF log background rewriting) is
    # performing a lot of I/O against the disk, in some Linux configurations
    # Redis may block too long on the fsync() call. Note that there is no fix for
    # this currently, as even performing fsync in a different thread will block
    # our synchronous write(2) call.
    #
    # In order to mitigate this problem it's possible to use the following option
    # that will prevent fsync() from being called in the main process while a
    # BGSAVE or BGREWRITEAOF is in progress.
    #
    # This means that while another child is saving, the durability of Redis is
    # the same as "appendfsync none". In practical terms, this means that it is
    # possible to lose up to 30 seconds of log in the worst scenario (with the
    # default Linux settings).
    #
    # If you have latency problems turn this to "yes". Otherwise leave it as
    # "no" that is the safest pick from the point of view of durability.

    no-appendfsync-on-rewrite no

    # Automatic rewrite of the append only file.
    # Redis is able to automatically rewrite the log file implicitly calling
    # BGREWRITEAOF when the AOF log size grows by the specified percentage.
    #
    # This is how it works: Redis remembers the size of the AOF file after the
    # latest rewrite (if no rewrite has happened since the restart, the size of
    # the AOF at startup is used).
    #
    # This base size is compared to the current size. If the current size is
    # bigger than the specified percentage, the rewrite is triggered. Also
    # you need to specify a minimal size for the AOF file to be rewritten, this
    # is useful to avoid rewriting the AOF file even if the percentage increase
    # is reached but it is still pretty small.
    #
    # Specify a percentage of zero in order to disable the automatic AOF
    # rewrite feature.

    auto-aof-rewrite-percentage 100
    auto-aof-rewrite-min-size 64mb

    # An AOF file may be found to be truncated at the end during the Redis
    # startup process, when the AOF data gets loaded back into memory.
    # This may happen when the system where Redis is running
    # crashes, especially when an ext4 filesystem is mounted without the
    # data=ordered option (however this can't happen when Redis itself
    # crashes or aborts but the operating system still works correctly).
    #
    # Redis can either exit with an error when this happens, or load as much
    # data as possible (the default now) and start if the AOF file is found
    # to be truncated at the end. The following option controls this behavior.
    #
    # If aof-load-truncated is set to yes, a truncated AOF file is loaded and
    # the Redis server starts emitting a log to inform the user of the event.
    # Otherwise if the option is set to no, the server aborts with an error
    # and refuses to start. When the option is set to no, the user requires
    # to fix the AOF file using the "redis-check-aof" utility before to restart
    # the server.
    #
    # Note that if the AOF file will be found to be corrupted in the middle
    # the server will still exit with an error. This option only applies when
    # Redis will try to read more data from the AOF file but not enough bytes
    # will be found.
    aof-load-truncated yes

    # When rewriting the AOF file, Redis is able to use an RDB preamble in the
    # AOF file for faster rewrites and recoveries. When this option is turned
    # on the rewritten AOF file is composed of two different stanzas:
    #
    #   [RDB file][AOF tail]
    #
    # When loading Redis recognizes that the AOF file starts with the "REDIS"
    # string and loads the prefixed RDB file, and continues loading the AOF
    # tail.
    #
    # This is currently turned off by default in order to avoid the surprise
    # of a format change, but will at some point be used as the default.
    aof-use-rdb-preamble no

    ################################ LUA SCRIPTING  ###############################

    # Max execution time of a Lua script in milliseconds.
    #
    # If the maximum execution time is reached Redis will log that a script is
    # still in execution after the maximum allowed time and will start to
    # reply to queries with an error.
    #
    # When a long running script exceeds the maximum execution time only the
    # SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be
    # used to stop a script that did not yet called write commands. The second
    # is the only way to shut down the server in the case a write command was
    # already issued by the script but the user doesn't want to wait for the natural
    # termination of the script.
    #
    # Set it to 0 or a negative value for unlimited execution without warnings.
    lua-time-limit 5000

    ################################ REDIS CLUSTER  ###############################
    #
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
    # WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however
    # in order to mark it as "mature" we need to wait for a non trivial percentage
    # of users to deploy it in production.
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
    #
    # Normal Redis instances can't be part of a Redis Cluster; only nodes that are
    # started as cluster nodes can. In order to start a Redis instance as a
    # cluster node enable the cluster support uncommenting the following:
    #
    # cluster-enabled yes

    # Every cluster node has a cluster configuration file. This file is not
    # intended to be edited by hand. It is created and updated by Redis nodes.
    # Every Redis Cluster node requires a different cluster configuration file.
    # Make sure that instances running in the same system do not have
    # overlapping cluster configuration file names.
    #
    # cluster-config-file nodes-6379.conf

    # Cluster node timeout is the amount of milliseconds a node must be unreachable
    # for it to be considered in failure state.
    # Most other internal time limits are multiple of the node timeout.
    #
    # cluster-node-timeout 15000

    # A slave of a failing master will avoid to start a failover if its data
    # looks too old.
    #
    # There is no simple way for a slave to actually have an exact measure of
    # its "data age", so the following two checks are performed:
    #
    # 1) If there are multiple slaves able to failover, they exchange messages
    #    in order to try to give an advantage to the slave with the best
    #    replication offset (more data from the master processed).
    #    Slaves will try to get their rank by offset, and apply to the start
    #    of the failover a delay proportional to their rank.
    #
    # 2) Every single slave computes the time of the last interaction with
    #    its master. This can be the last ping or command received (if the master
    #    is still in the "connected" state), or the time that elapsed since the
    #    disconnection with the master (if the replication link is currently down).
    #    If the last interaction is too old, the slave will not try to failover
    #    at all.
    #
    # The point "2" can be tuned by user. Specifically a slave will not perform
    # the failover if, since the last interaction with the master, the time
    # elapsed is greater than:
    #
    #   (node-timeout * slave-validity-factor) + repl-ping-slave-period
    #
    # So for example if node-timeout is 30 seconds, and the slave-validity-factor
    # is 10, and assuming a default repl-ping-slave-period of 10 seconds, the
    # slave will not try to failover if it was not able to talk with the master
    # for longer than 310 seconds.
    #
    # A large slave-validity-factor may allow slaves with too old data to failover
    # a master, while a too small value may prevent the cluster from being able to
    # elect a slave at all.
    #
    # For maximum availability, it is possible to set the slave-validity-factor
    # to a value of 0, which means, that slaves will always try to failover the
    # master regardless of the last time they interacted with the master.
    # (However they'll always try to apply a delay proportional to their
    # offset rank).
    #
    # Zero is the only value able to guarantee that when all the partitions heal
    # the cluster will always be able to continue.
    #
    # cluster-slave-validity-factor 10

    # Cluster slaves are able to migrate to orphaned masters, that are masters
    # that are left without working slaves. This improves the cluster ability
    # to resist to failures as otherwise an orphaned master can't be failed over
    # in case of failure if it has no working slaves.
    #
    # Slaves migrate to orphaned masters only if there are still at least a
    # given number of other working slaves for their old master. This number
    # is the "migration barrier". A migration barrier of 1 means that a slave
    # will migrate only if there is at least 1 other working slave for its master
    # and so forth. It usually reflects the number of slaves you want for every
    # master in your cluster.
    #
    # Default is 1 (slaves migrate only if their masters remain with at least
    # one slave). To disable migration just set it to a very large value.
    # A value of 0 can be set but is useful only for debugging and dangerous
    # in production.
    #
    # cluster-migration-barrier 1

    # By default Redis Cluster nodes stop accepting queries if they detect there
    # is at least an hash slot uncovered (no available node is serving it).
    # This way if the cluster is partially down (for example a range of hash slots
    # are no longer covered) all the cluster becomes, eventually, unavailable.
    # It automatically returns available as soon as all the slots are covered again.
    #
    # However sometimes you want the subset of the cluster which is working,
    # to continue to accept queries for the part of the key space that is still
    # covered. In order to do so, just set the cluster-require-full-coverage
    # option to no.
    #
    # cluster-require-full-coverage yes

    # This option, when set to yes, prevents slaves from trying to failover its
    # master during master failures. However the master can still perform a
    # manual failover, if forced to do so.
    #
    # This is useful in different scenarios, especially in the case of multiple
    # data center operations, where we want one side to never be promoted if not
    # in the case of a total DC failure.
    #
    # cluster-slave-no-failover no

    # In order to setup your cluster make sure to read the documentation
    # available at http://redis.io web site.

    ########################## CLUSTER DOCKER/NAT support  ########################

    # In certain deployments, Redis Cluster nodes address discovery fails, because
    # addresses are NAT-ted or because ports are forwarded (the typical case is
    # Docker and other containers).
    #
    # In order to make Redis Cluster working in such environments, a static
    # configuration where each node knows its public address is needed. The
    # following two options are used for this scope, and are:
    #
    # * cluster-announce-ip
    # * cluster-announce-port
    # * cluster-announce-bus-port
    #
    # Each instruct the node about its address, client port, and cluster message
    # bus port. The information is then published in the header of the bus packets
    # so that other nodes will be able to correctly map the address of the node
    # publishing the information.
    #
    # If the above options are not used, the normal Redis Cluster auto-detection
    # will be used instead.
    #
    # Note that when remapped, the bus port may not be at the fixed offset of
    # clients port + 10000, so you can specify any port and bus-port depending
    # on how they get remapped. If the bus-port is not set, a fixed offset of
    # 10000 will be used as usually.
    #
    # Example:
    #
    # cluster-announce-ip 10.1.1.5
    # cluster-announce-port 6379
    # cluster-announce-bus-port 6380

    ################################## SLOW LOG ###################################

    # The Redis Slow Log is a system to log queries that exceeded a specified
    # execution time. The execution time does not include the I/O operations
    # like talking with the client, sending the reply and so forth,
    # but just the time needed to actually execute the command (this is the only
    # stage of command execution where the thread is blocked and can not serve
    # other requests in the meantime).
    #
    # You can configure the slow log with two parameters: one tells Redis
    # what is the execution time, in microseconds, to exceed in order for the
    # command to get logged, and the other parameter is the length of the
    # slow log. When a new command is logged the oldest one is removed from the
    # queue of logged commands.

    # The following time is expressed in microseconds, so 1000000 is equivalent
    # to one second. Note that a negative number disables the slow log, while
    # a value of zero forces the logging of every command.
    slowlog-log-slower-than 10000

    # There is no limit to this length. Just be aware that it will consume memory.
    # You can reclaim memory used by the slow log with SLOWLOG RESET.
    slowlog-max-len 128

    ################################ LATENCY MONITOR ##############################

    # The Redis latency monitoring subsystem samples different operations
    # at runtime in order to collect data related to possible sources of
    # latency of a Redis instance.
    #
    # Via the LATENCY command this information is available to the user that can
    # print graphs and obtain reports.
    #
    # The system only logs operations that were performed in a time equal or
    # greater than the amount of milliseconds specified via the
    # latency-monitor-threshold configuration directive. When its value is set
    # to zero, the latency monitor is turned off.
    #
    # By default latency monitoring is disabled since it is mostly not needed
    # if you don't have latency issues, and collecting data has a performance
    # impact, that while very small, can be measured under big load. Latency
    # monitoring can easily be enabled at runtime using the command
    # "CONFIG SET latency-monitor-threshold <milliseconds>" if needed.
    latency-monitor-threshold 0

    ############################# EVENT NOTIFICATION ##############################

    # Redis can notify Pub/Sub clients about events happening in the key space.
    # This feature is documented at http://redis.io/topics/notifications
    #
    # For instance if keyspace events notification is enabled, and a client
    # performs a DEL operation on key "foo" stored in the Database 0, two
    # messages will be published via Pub/Sub:
    #
    # PUBLISH __keyspace@0__:foo del
    # PUBLISH __keyevent@0__:del foo
    #
    # It is possible to select the events that Redis will notify among a set
    # of classes. Every class is identified by a single character:
    #
    #  K     Keyspace events, published with __keyspace@<db>__ prefix.
    #  E     Keyevent events, published with __keyevent@<db>__ prefix.
    #  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...
    #  $     String commands
    #  l     List commands
    #  s     Set commands
    #  h     Hash commands
    #  z     Sorted set commands
    #  x     Expired events (events generated every time a key expires)
    #  e     Evicted events (events generated when a key is evicted for maxmemory)
    #  A     Alias for g$lshzxe, so that the "AKE" string means all the events.
    #
    #  The "notify-keyspace-events" takes as argument a string that is composed
    #  of zero or multiple characters. The empty string means that notifications
    #  are disabled.
    #
    #  Example: to enable list and generic events, from the point of view of the
    #           event name, use:
    #
    #  notify-keyspace-events Elg
    #
    #  Example 2: to get the stream of the expired keys subscribing to channel
    #             name __keyevent@0__:expired use:
    #
    #  notify-keyspace-events Ex
    #
    #  By default all notifications are disabled because most users don't need
    #  this feature and the feature has some overhead. Note that if you don't
    #  specify at least one of K or E, no events will be delivered.
    notify-keyspace-events ""

    ############################### ADVANCED CONFIG ###############################

    # Hashes are encoded using a memory efficient data structure when they have a
    # small number of entries, and the biggest entry does not exceed a given
    # threshold. These thresholds can be configured using the following directives.
    hash-max-ziplist-entries 512
    hash-max-ziplist-value 64

    # Lists are also encoded in a special way to save a lot of space.
    # The number of entries allowed per internal list node can be specified
    # as a fixed maximum size or a maximum number of elements.
    # For a fixed maximum size, use -5 through -1, meaning:
    # -5: max size: 64 Kb  <-- not recommended for normal workloads
    # -4: max size: 32 Kb  <-- not recommended
    # -3: max size: 16 Kb  <-- probably not recommended
    # -2: max size: 8 Kb   <-- good
    # -1: max size: 4 Kb   <-- good
    # Positive numbers mean store up to _exactly_ that number of elements
    # per list node.
    # The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),
    # but if your use case is unique, adjust the settings as necessary.
    list-max-ziplist-size -2

    # Lists may also be compressed.
    # Compress depth is the number of quicklist ziplist nodes from *each* side of
    # the list to *exclude* from compression.  The head and tail of the list
    # are always uncompressed for fast push/pop operations.  Settings are:
    # 0: disable all list compression
    # 1: depth 1 means "don't start compressing until after 1 node into the list,
    #    going from either the head or tail"
    #    So: [head]->node->node->...->node->[tail]
    #    [head], [tail] will always be uncompressed; inner nodes will compress.
    # 2: [head]->[next]->node->node->...->node->[prev]->[tail]
    #    2 here means: don't compress head or head->next or tail->prev or tail,
    #    but compress all nodes between them.
    # 3: [head]->[next]->[next]->node->node->...->node->[prev]->[prev]->[tail]
    # etc.
    list-compress-depth 0

    # Sets have a special encoding in just one case: when a set is composed
    # of just strings that happen to be integers in radix 10 in the range
    # of 64 bit signed integers.
    # The following configuration setting sets the limit in the size of the
    # set in order to use this special memory saving encoding.
    set-max-intset-entries 512

    # Similarly to hashes and lists, sorted sets are also specially encoded in
    # order to save a lot of space. This encoding is only used when the length and
    # elements of a sorted set are below the following limits:
    zset-max-ziplist-entries 128
    zset-max-ziplist-value 64

    # HyperLogLog sparse representation bytes limit. The limit includes the
    # 16 bytes header. When an HyperLogLog using the sparse representation crosses
    # this limit, it is converted into the dense representation.
    #
    # A value greater than 16000 is totally useless, since at that point the
    # dense representation is more memory efficient.
    #
    # The suggested value is ~ 3000 in order to have the benefits of
    # the space efficient encoding without slowing down too much PFADD,
    # which is O(N) with the sparse encoding. The value can be raised to
    # ~ 10000 when CPU is not a concern, but space is, and the data set is
    # composed of many HyperLogLogs with cardinality in the 0 - 15000 range.
    hll-sparse-max-bytes 3000

    # Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in
    # order to help rehashing the main Redis hash table (the one mapping top-level
    # keys to values). The hash table implementation Redis uses (see dict.c)
    # performs a lazy rehashing: the more operation you run into a hash table
    # that is rehashing, the more rehashing "steps" are performed, so if the
    # server is idle the rehashing is never complete and some more memory is used
    # by the hash table.
    #
    # The default is to use this millisecond 10 times every second in order to
    # actively rehash the main dictionaries, freeing memory when possible.
    #
    # If unsure:
    # use "activerehashing no" if you have hard latency requirements and it is
    # not a good thing in your environment that Redis can reply from time to time
    # to queries with 2 milliseconds delay.
    #
    # use "activerehashing yes" if you don't have such hard requirements but
    # want to free memory asap when possible.
    activerehashing yes

    # The client output buffer limits can be used to force disconnection of clients
    # that are not reading data from the server fast enough for some reason (a
    # common reason is that a Pub/Sub client can't consume messages as fast as the
    # publisher can produce them).
    #
    # The limit can be set differently for the three different classes of clients:
    #
    # normal -> normal clients including MONITOR clients
    # slave  -> slave clients
    # pubsub -> clients subscribed to at least one pubsub channel or pattern
    #
    # The syntax of every client-output-buffer-limit directive is the following:
    #
    # client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds>
    #
    # A client is immediately disconnected once the hard limit is reached, or if
    # the soft limit is reached and remains reached for the specified number of
    # seconds (continuously).
    # So for instance if the hard limit is 32 megabytes and the soft limit is
    # 16 megabytes / 10 seconds, the client will get disconnected immediately
    # if the size of the output buffers reach 32 megabytes, but will also get
    # disconnected if the client reaches 16 megabytes and continuously overcomes
    # the limit for 10 seconds.
    #
    # By default normal clients are not limited because they don't receive data
    # without asking (in a push way), but just after a request, so only
    # asynchronous clients may create a scenario where data is requested faster
    # than it can read.
    #
    # Instead there is a default limit for pubsub and slave clients, since
    # subscribers and slaves receive data in a push fashion.
    #
    # Both the hard or the soft limit can be disabled by setting them to zero.
    client-output-buffer-limit normal 0 0 0
    client-output-buffer-limit slave 256mb 64mb 60
    client-output-buffer-limit pubsub 32mb 8mb 60

    # Client query buffers accumulate new commands. They are limited to a fixed
    # amount by default in order to avoid that a protocol desynchronization (for
    # instance due to a bug in the client) will lead to unbound memory usage in
    # the query buffer. However you can configure it here if you have very special
    # needs, such us huge multi/exec requests or alike.
    #
    # client-query-buffer-limit 1gb

    # In the Redis protocol, bulk requests, that are, elements representing single
    # strings, are normally limited ot 512 mb. However you can change this limit
    # here.
    #
    # proto-max-bulk-len 512mb

    # Redis calls an internal function to perform many background tasks, like
    # closing connections of clients in timeout, purging expired keys that are
    # never requested, and so forth.
    #
    # Not all tasks are performed with the same frequency, but Redis checks for
    # tasks to perform according to the specified "hz" value.
    #
    # By default "hz" is set to 10. Raising the value will use more CPU when
    # Redis is idle, but at the same time will make Redis more responsive when
    # there are many keys expiring at the same time, and timeouts may be
    # handled with more precision.
    #
    # The range is between 1 and 500, however a value over 100 is usually not
    # a good idea. Most users should use the default of 10 and raise this up to
    # 100 only in environments where very low latency is required.
    hz 10

    # When a child rewrites the AOF file, if the following option is enabled
    # the file will be fsync-ed every 32 MB of data generated. This is useful
    # in order to commit the file to the disk more incrementally and avoid
    # big latency spikes.
    aof-rewrite-incremental-fsync yes

    # Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good
    # idea to start with the default settings and only change them after investigating
    # how to improve the performances and how the keys LFU change over time, which
    # is possible to inspect via the OBJECT FREQ command.
    #
    # There are two tunable parameters in the Redis LFU implementation: the
    # counter logarithm factor and the counter decay time. It is important to
    # understand what the two parameters mean before changing them.
    #
    # The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis
    # uses a probabilistic increment with logarithmic behavior. Given the value
    # of the old counter, when a key is accessed, the counter is incremented in
    # this way:
    #
    # 1. A random number R between 0 and 1 is extracted.
    # 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).
    # 3. The counter is incremented only if R < P.
    #
    # The default lfu-log-factor is 10. This is a table of how the frequency
    # counter changes with a different number of accesses with different
    # logarithmic factors:
    #
    # +--------+------------+------------+------------+------------+------------+
    # | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   |
    # +--------+------------+------------+------------+------------+------------+
    # | 0      | 104        | 255        | 255        | 255        | 255        |
    # +--------+------------+------------+------------+------------+------------+
    # | 1      | 18         | 49         | 255        | 255        | 255        |
    # +--------+------------+------------+------------+------------+------------+
    # | 10     | 10         | 18         | 142        | 255        | 255        |
    # +--------+------------+------------+------------+------------+------------+
    # | 100    | 8          | 11         | 49         | 143        | 255        |
    # +--------+------------+------------+------------+------------+------------+
    #
    # NOTE: The above table was obtained by running the following commands:
    #
    #   redis-benchmark -n 1000000 incr foo
    #   redis-cli object freq foo
    #
    # NOTE 2: The counter initial value is 5 in order to give new objects a chance
    # to accumulate hits.
    #
    # The counter decay time is the time, in minutes, that must elapse in order
    # for the key counter to be divided by two (or decremented if it has a value
    # less <= 10).
    #
    # The default value for the lfu-decay-time is 1. A Special value of 0 means to
    # decay the counter every time it happens to be scanned.
    #
    # lfu-log-factor 10
    # lfu-decay-time 1

    ########################### ACTIVE DEFRAGMENTATION #######################
    #
    # WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested
    # even in production and manually tested by multiple engineers for some
    # time.
    #
    # What is active defragmentation?
    # -------------------------------
    #
    # Active (online) defragmentation allows a Redis server to compact the
    # spaces left between small allocations and deallocations of data in memory,
    # thus allowing to reclaim back memory.
    #
    # Fragmentation is a natural process that happens with every allocator (but
    # less so with Jemalloc, fortunately) and certain workloads. Normally a server
    # restart is needed in order to lower the fragmentation, or at least to flush
    # away all the data and create it again. However thanks to this feature
    # implemented by Oran Agra for Redis 4.0 this process can happen at runtime
    # in an "hot" way, while the server is running.
    #
    # Basically when the fragmentation is over a certain level (see the
    # configuration options below) Redis will start to create new copies of the
    # values in contiguous memory regions by exploiting certain specific Jemalloc
    # features (in order to understand if an allocation is causing fragmentation
    # and to allocate it in a better place), and at the same time, will release the
    # old copies of the data. This process, repeated incrementally for all the keys
    # will cause the fragmentation to drop back to normal values.
    #
    # Important things to understand:
    #
    # 1. This feature is disabled by default, and only works if you compiled Redis
    #    to use the copy of Jemalloc we ship with the source code of Redis.
    #    This is the default with Linux builds.
    #
    # 2. You never need to enable this feature if you don't have fragmentation
    #    issues.
    #
    # 3. Once you experience fragmentation, you can enable this feature when
    #    needed with the command "CONFIG SET activedefrag yes".
    #
    # The configuration parameters are able to fine tune the behavior of the
    # defragmentation process. If you are not sure about what they mean it is
    # a good idea to leave the defaults untouched.

    # Enabled active defragmentation
    # activedefrag yes

    # Minimum amount of fragmentation waste to start active defrag
    # active-defrag-ignore-bytes 100mb

    # Minimum percentage of fragmentation to start active defrag
    # active-defrag-threshold-lower 10

    # Maximum percentage of fragmentation at which we use maximum effort
    # active-defrag-threshold-upper 100

    # Minimal effort for defrag in CPU percentage
    # active-defrag-cycle-min 25

    # Maximal effort for defrag in CPU percentage
    # active-defrag-cycle-max 75
---
# Source: 0chain/templates/0dns-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-0chain-0dns
  labels:
    app.kubernetes.io/component: 0dns
spec:
  type: ClusterIP
  ports:
    - port: 9091
      targetPort: 9091
      protocol: TCP
      name: http9091
  selector:
    app.kubernetes.io/component: 0dns
---
# Source: 0chain/templates/blobber-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-0chain-blobber-0
  labels:
    app.kubernetes.io/component: blobber
    app.kubernetes.io/part-of: blobber-0
spec:
  type: ClusterIP
  ports:
    - port: 3130
      targetPort: 31300
      protocol: TCP
      name: http31300
  selector:
    app.kubernetes.io/component: blobber
    app.kubernetes.io/part-of: blobber-0
---
# Source: 0chain/templates/blobber-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-0chain-blobber-1
  labels:
    app.kubernetes.io/component: blobber
    app.kubernetes.io/part-of: blobber-1
spec:
  type: ClusterIP
  ports:
    - port: 3130
      targetPort: 31301
      protocol: TCP
      name: http31301
  selector:
    app.kubernetes.io/component: blobber
    app.kubernetes.io/part-of: blobber-1
---
# Source: 0chain/templates/cassandra-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-0chain-cassandra
spec:
  clusterIP: None
  selector:
    app.kubernetes.io/component: RELEASE-NAME-0chain-cassandra
  ports:
  - protocol: TCP
    port: 9042
    targetPort: 9042
---
# Source: 0chain/templates/miner-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-0chain-miner-0
  labels:
    app.kubernetes.io/component: miner
    app.kubernetes.io/part-of: miner-0
spec:
  type: ClusterIP
  ports:
    - port: 7071
      targetPort: 7071
      protocol: TCP
      name: http7071
  selector:
    app.kubernetes.io/component: miner
    app.kubernetes.io/part-of: miner-0
---
# Source: 0chain/templates/miner-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-0chain-miner-1
  labels:
    app.kubernetes.io/component: miner
    app.kubernetes.io/part-of: miner-1
spec:
  type: ClusterIP
  ports:
    - port: 7071
      targetPort: 7071
      protocol: TCP
      name: http7071
  selector:
    app.kubernetes.io/component: miner
    app.kubernetes.io/part-of: miner-1
---
# Source: 0chain/templates/miner-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-0chain-miner-2
  labels:
    app.kubernetes.io/component: miner
    app.kubernetes.io/part-of: miner-2
spec:
  type: ClusterIP
  ports:
    - port: 7071
      targetPort: 7071
      protocol: TCP
      name: http7071
  selector:
    app.kubernetes.io/component: miner
    app.kubernetes.io/part-of: miner-2
---
# Source: 0chain/templates/miner-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-0chain-miner-3
  labels:
    app.kubernetes.io/component: miner
    app.kubernetes.io/part-of: miner-3
spec:
  type: ClusterIP
  ports:
    - port: 7071
      targetPort: 7071
      protocol: TCP
      name: http7071
  selector:
    app.kubernetes.io/component: miner
    app.kubernetes.io/part-of: miner-3
---
# Source: 0chain/templates/mongo-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-0chain-mongo
  labels:
    app.kubernetes.io/component: RELEASE-NAME-0chain-mongo
spec:
  type: ClusterIP
  ports:
    - port: 27017
      targetPort: 27017
      protocol: TCP
      name: http27017
  selector:
    app.kubernetes.io/component: RELEASE-NAME-0chain-mongo
---
# Source: 0chain/templates/postgres-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-0chain-postgres
spec:
  clusterIP: None
  selector:
    app.kubernetes.io/component: RELEASE-NAME-0chain-postgres
  ports:
  - protocol: TCP
    port: 5432
    targetPort: 5432
---
# Source: 0chain/templates/sharder-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-0chain-sharder-0
  labels:
    app.kubernetes.io/component: sharder
    app.kubernetes.io/part-of: sharder-0
spec:
  type: ClusterIP
  ports:
    - port: 7171
      targetPort: 7171
      protocol: TCP
      name: http7171
  selector:
    app.kubernetes.io/component: sharder
    app.kubernetes.io/part-of: sharder-0
---
# Source: 0chain/templates/validator-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-0chain-validator-0
  labels:
    app.kubernetes.io/component: validator
    app.kubernetes.io/part-of: validator-0
spec:
  type: ClusterIP
  ports:
    - port: 5051
      targetPort: 5051
      protocol: TCP
      name: http5051
  selector:
    app.kubernetes.io/component: validator
    app.kubernetes.io/part-of: validator-0
---
# Source: 0chain/templates/0dns-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: RELEASE-NAME-0chain-0dns
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: 0dns
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: 0chain
      app.kubernetes.io/instance: RELEASE-NAME
      app.kubernetes.io/component: 0dns
  template:
    metadata:
      labels:
        app.kubernetes.io/name: 0chain
        app.kubernetes.io/instance: RELEASE-NAME
        app.kubernetes.io/component: 0dns
    spec:
      serviceAccountName: RELEASE-NAME-0chain
      securityContext:
        {}
      containers:
        - name: 0chain-0dns
          securityContext:
            {}
          image: "0chaintest/0dns:latest"
          imagePullPolicy: IfNotPresent
          args:
            - ./bin/zdns
            - --deployment_mode
            - "0"
            - --magic_block
            - /0dns/config/magicBlock.json
          env:
            - name: DOCKER
              value: "true"
          ports:
            - name: http
              containerPort: 7171
              protocol: TCP
          # livenessProbe:
            # httpGet:
              # path: /
              # port: http
          # readinessProbe:
            # httpGet:
              # path: /
              # port: http
          resources:
            {}
          volumeMounts:
          - mountPath: /0dns/config/magicBlock.json
            name: config
            subPath: magicBlock.json
          - mountPath: /0dns/config/0dns.yaml
            name: 0dns-config
            subPath: 0dns.yaml
      initContainers:
        - name: 0chain-mongo-setup
          image: mongo
          command:
            - /bin/sh
            - -ce
            - |
              apt update || true && apt install netcat -y
              while ! nc -z RELEASE-NAME-0chain-mongo 27017; do echo 'waiting for cassandra service to be up'; sleep 1; done
              mongo 'mongodb://RELEASE-NAME-0chain-mongo:27017' --eval 'rs.initiate({_id : "rs0", members: [ { _id: 0, host: "RELEASE-NAME-0chain-mongo:27017" } ] })'
      volumes:
        - name: 0dns-config
          configMap:
            name: RELEASE-NAME-0chain-0dns
        - name: config
          configMap:
            name: RELEASE-NAME-0chain
---
# Source: 0chain/templates/blobber-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: RELEASE-NAME-0chain-blobber-0
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: blobber
    app.kubernetes.io/part-of: blobber-0
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: 0chain
      app.kubernetes.io/instance: RELEASE-NAME
      app.kubernetes.io/component: blobber
      app.kubernetes.io/part-of: blobber-0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: 0chain
        app.kubernetes.io/instance: RELEASE-NAME
        app.kubernetes.io/component: blobber
        app.kubernetes.io/part-of: blobber-0
    spec:
      serviceAccountName: RELEASE-NAME-0chain
      securityContext:
        {}
      containers:
        - name: 0chain
          securityContext:
            {}
          image: "0chaindev/blobber:latest"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - ./bin/blobber --port "31300" --hostname v1.load.testnet-0chain.net --deployment_mode "0" --keys_file config/b0bnode0_keys.txt
              --files_dir /blobber/files --log_dir /blobber/log --db_dir /blobber/data --minio_file config/minio_config.txt
          env:
            - name: DOCKER
              value: "true"
            - name: DB_NAME
              value: blobber_meta
            - name: DB_USER
              value: blobber_user
            - name: DB_PASSWORD
              value: blobber
            - name: DB_HOST
              value: RELEASE-NAME-0chain-postgres-0.RELEASE-NAME-0chain-postgres
            - name: DB_PORT
              value: "5432"
          ports:
            - name: http3130
              containerPort: 3130
              protocol: TCP
          resources:
            {}
          volumeMounts:
            - name: files
              mountPath: /blobber/files
            - name: log
              mountPath: /blobber/log
            - mountPath: /blobber/config/
              name: blobber-config
      initContainers:
        - name: 0chain-postgres-setup
          image: postgres:11
          command:
            - /bin/bash
            - -ce
            - |
              apt update || true && apt install git -y
              git clone https://github.com/0chain/blobber.git /blobber

              psql=( psql --username "$POSTGRES_USER" --port "$POSTGRES_PORT" --host "$POSTGRES_HOST" )

              until pg_isready -h $POSTGRES_HOST
              do
                echo "Sleep 1s and try again..."
                sleep 1
              done

              for f in /blobber/sql/*; do
                case "$f" in
                  *.sh)     echo "$0: running $f"; . "$f" ;;
                  *.sql)    echo "$0: running $f"; "${psql[@]}" -f "$f"; echo ;;
                  *.sql.gz) echo "$0: running $f"; gunzip -c "$f" | "${psql[@]}"; echo ;;
                  *)        echo "$0: ignoring $f" ;;
                esac
                echo
              done
          env:
            - name: POSTGRES_HOST
              value: RELEASE-NAME-0chain-postgres-0.RELEASE-NAME-0chain-postgres
            - name: POSTGRES_PORT
              value: "5432"
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_HOST_AUTH_METHOD
              value: trust
      volumes:
        - name: log
          emptyDir: {}
        - name: files
          emptyDir: {}
        - name: blobber-config
          configMap:
            name: RELEASE-NAME-0chain-blobber
---
# Source: 0chain/templates/blobber-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: RELEASE-NAME-0chain-blobber-1
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: blobber
    app.kubernetes.io/part-of: blobber-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: 0chain
      app.kubernetes.io/instance: RELEASE-NAME
      app.kubernetes.io/component: blobber
      app.kubernetes.io/part-of: blobber-1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: 0chain
        app.kubernetes.io/instance: RELEASE-NAME
        app.kubernetes.io/component: blobber
        app.kubernetes.io/part-of: blobber-1
    spec:
      serviceAccountName: RELEASE-NAME-0chain
      securityContext:
        {}
      containers:
        - name: 0chain
          securityContext:
            {}
          image: "0chaindev/blobber:latest"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - ./bin/blobber --port "31301" --hostname v1.load.testnet-0chain.net --deployment_mode "0" --keys_file config/b0bnode1_keys.txt
              --files_dir /blobber/files --log_dir /blobber/log --db_dir /blobber/data --minio_file config/minio_config.txt
          env:
            - name: DOCKER
              value: "true"
            - name: DB_NAME
              value: blobber_meta
            - name: DB_USER
              value: blobber_user
            - name: DB_PASSWORD
              value: blobber
            - name: DB_HOST
              value: RELEASE-NAME-0chain-postgres-1.RELEASE-NAME-0chain-postgres
            - name: DB_PORT
              value: "5432"
          ports:
            - name: http3130
              containerPort: 3130
              protocol: TCP
          resources:
            {}
          volumeMounts:
            - name: files
              mountPath: /blobber/files
            - name: log
              mountPath: /blobber/log
            - mountPath: /blobber/config/
              name: blobber-config
      initContainers:
        - name: 0chain-postgres-setup
          image: postgres:11
          command:
            - /bin/bash
            - -ce
            - |
              apt update || true && apt install git -y
              git clone https://github.com/0chain/blobber.git /blobber

              psql=( psql --username "$POSTGRES_USER" --port "$POSTGRES_PORT" --host "$POSTGRES_HOST" )

              until pg_isready -h $POSTGRES_HOST
              do
                echo "Sleep 1s and try again..."
                sleep 1
              done

              for f in /blobber/sql/*; do
                case "$f" in
                  *.sh)     echo "$0: running $f"; . "$f" ;;
                  *.sql)    echo "$0: running $f"; "${psql[@]}" -f "$f"; echo ;;
                  *.sql.gz) echo "$0: running $f"; gunzip -c "$f" | "${psql[@]}"; echo ;;
                  *)        echo "$0: ignoring $f" ;;
                esac
                echo
              done
          env:
            - name: POSTGRES_HOST
              value: RELEASE-NAME-0chain-postgres-1.RELEASE-NAME-0chain-postgres
            - name: POSTGRES_PORT
              value: "5432"
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_HOST_AUTH_METHOD
              value: trust
      volumes:
        - name: log
          emptyDir: {}
        - name: files
          emptyDir: {}
        - name: blobber-config
          configMap:
            name: RELEASE-NAME-0chain-blobber
---
# Source: 0chain/templates/miner-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: RELEASE-NAME-0chain-miner-0
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: miner
    app.kubernetes.io/part-of: miner-0
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: 0chain
      app.kubernetes.io/instance: RELEASE-NAME
      app.kubernetes.io/component: miner
      app.kubernetes.io/part-of: miner-0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: 0chain
        app.kubernetes.io/instance: RELEASE-NAME
        app.kubernetes.io/component: miner
        app.kubernetes.io/part-of: miner-0
    spec:
      serviceAccountName: RELEASE-NAME-0chain
      securityContext:
        {}
      containers:
        - name: 0chain
          securityContext:
            {}
          image: "0chaindev/miner:latest"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -ce
            - |
              mkdir -p /0chain/data/rocksdb &&
              while ! nc -z 127.0.0.1 6379; do echo "waiting for redis state service to be up"; sleep 1; done &&
              while ! nc -z 127.0.0.1 6479; do echo "waiting for redis transaction service to be up"; sleep 1; done &&
              ./bin/miner --deployment_mode 0 --keys_file config/b0mnode0_keys.txt
          env:
            - name: DOCKER
              value: ""
            - name: REDIS_HOST
              value: 127.0.0.1
            - name: REDIS_TXNS
              value: 127.0.0.1
          ports:
            - name: http
              containerPort: 7071
              protocol: TCP
          # livenessProbe:
            # httpGet:
              # path: /
              # port: http
          # readinessProbe:
            # httpGet:
              # path: /
              # port: http
          resources:
            {}
          volumeMounts:
            - name: data
              mountPath: /0chain/data
            - name: config
              mountPath: /0chain/config
        - name: 0chain-redis
          image: redis:alpine
          args:
            - redis-server
            - /0chain/config/redis/state.redis.conf
          volumeMounts:
            - name: data
              mountPath: /0chain/data
            - name: redis-config
              mountPath: /0chain/config/redis
        - name: 0chain-redis-txns
          image: redis:alpine
          args:
            - redis-server
            - /0chain/config/redis/transactions.redis.conf
          volumeMounts:
            - name: data
              mountPath: /0chain/data
            - name: redis-config
              mountPath: /0chain/config/redis
      initContainers:
        - name: 0chain-redis-setup
          image: cassandra:3.11.4
          command:
            - /bin/sh
            - -ce
            - |
              mkdir -p /0chain/data/redis/transactions
              chmod -R 777 /0chain/data/redis
              mkdir -p /0chain/data/redis/state
              chmod -R 777 /0chain/data/redis
          volumeMounts:
            - name: data
              mountPath: /0chain/data
      volumes:
        - name: data
          emptyDir: {}
        - name: config
          configMap:
            name: RELEASE-NAME-0chain
        - name: redis-config
          configMap:
            name: RELEASE-NAME-0chain-redis
---
# Source: 0chain/templates/miner-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: RELEASE-NAME-0chain-miner-1
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: miner
    app.kubernetes.io/part-of: miner-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: 0chain
      app.kubernetes.io/instance: RELEASE-NAME
      app.kubernetes.io/component: miner
      app.kubernetes.io/part-of: miner-1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: 0chain
        app.kubernetes.io/instance: RELEASE-NAME
        app.kubernetes.io/component: miner
        app.kubernetes.io/part-of: miner-1
    spec:
      serviceAccountName: RELEASE-NAME-0chain
      securityContext:
        {}
      containers:
        - name: 0chain
          securityContext:
            {}
          image: "0chaindev/miner:latest"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -ce
            - |
              mkdir -p /0chain/data/rocksdb &&
              while ! nc -z 127.0.0.1 6379; do echo "waiting for redis state service to be up"; sleep 1; done &&
              while ! nc -z 127.0.0.1 6479; do echo "waiting for redis transaction service to be up"; sleep 1; done &&
              ./bin/miner --deployment_mode 0 --keys_file config/b0mnode1_keys.txt
          env:
            - name: DOCKER
              value: ""
            - name: REDIS_HOST
              value: 127.0.0.1
            - name: REDIS_TXNS
              value: 127.0.0.1
          ports:
            - name: http
              containerPort: 7071
              protocol: TCP
          # livenessProbe:
            # httpGet:
              # path: /
              # port: http
          # readinessProbe:
            # httpGet:
              # path: /
              # port: http
          resources:
            {}
          volumeMounts:
            - name: data
              mountPath: /0chain/data
            - name: config
              mountPath: /0chain/config
        - name: 0chain-redis
          image: redis:alpine
          args:
            - redis-server
            - /0chain/config/redis/state.redis.conf
          volumeMounts:
            - name: data
              mountPath: /0chain/data
            - name: redis-config
              mountPath: /0chain/config/redis
        - name: 0chain-redis-txns
          image: redis:alpine
          args:
            - redis-server
            - /0chain/config/redis/transactions.redis.conf
          volumeMounts:
            - name: data
              mountPath: /0chain/data
            - name: redis-config
              mountPath: /0chain/config/redis
      initContainers:
        - name: 0chain-redis-setup
          image: cassandra:3.11.4
          command:
            - /bin/sh
            - -ce
            - |
              mkdir -p /0chain/data/redis/transactions
              chmod -R 777 /0chain/data/redis
              mkdir -p /0chain/data/redis/state
              chmod -R 777 /0chain/data/redis
          volumeMounts:
            - name: data
              mountPath: /0chain/data
      volumes:
        - name: data
          emptyDir: {}
        - name: config
          configMap:
            name: RELEASE-NAME-0chain
        - name: redis-config
          configMap:
            name: RELEASE-NAME-0chain-redis
---
# Source: 0chain/templates/miner-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: RELEASE-NAME-0chain-miner-2
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: miner
    app.kubernetes.io/part-of: miner-2
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: 0chain
      app.kubernetes.io/instance: RELEASE-NAME
      app.kubernetes.io/component: miner
      app.kubernetes.io/part-of: miner-2
  template:
    metadata:
      labels:
        app.kubernetes.io/name: 0chain
        app.kubernetes.io/instance: RELEASE-NAME
        app.kubernetes.io/component: miner
        app.kubernetes.io/part-of: miner-2
    spec:
      serviceAccountName: RELEASE-NAME-0chain
      securityContext:
        {}
      containers:
        - name: 0chain
          securityContext:
            {}
          image: "0chaindev/miner:latest"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -ce
            - |
              mkdir -p /0chain/data/rocksdb &&
              while ! nc -z 127.0.0.1 6379; do echo "waiting for redis state service to be up"; sleep 1; done &&
              while ! nc -z 127.0.0.1 6479; do echo "waiting for redis transaction service to be up"; sleep 1; done &&
              ./bin/miner --deployment_mode 0 --keys_file config/b0mnode2_keys.txt
          env:
            - name: DOCKER
              value: ""
            - name: REDIS_HOST
              value: 127.0.0.1
            - name: REDIS_TXNS
              value: 127.0.0.1
          ports:
            - name: http
              containerPort: 7071
              protocol: TCP
          # livenessProbe:
            # httpGet:
              # path: /
              # port: http
          # readinessProbe:
            # httpGet:
              # path: /
              # port: http
          resources:
            {}
          volumeMounts:
            - name: data
              mountPath: /0chain/data
            - name: config
              mountPath: /0chain/config
        - name: 0chain-redis
          image: redis:alpine
          args:
            - redis-server
            - /0chain/config/redis/state.redis.conf
          volumeMounts:
            - name: data
              mountPath: /0chain/data
            - name: redis-config
              mountPath: /0chain/config/redis
        - name: 0chain-redis-txns
          image: redis:alpine
          args:
            - redis-server
            - /0chain/config/redis/transactions.redis.conf
          volumeMounts:
            - name: data
              mountPath: /0chain/data
            - name: redis-config
              mountPath: /0chain/config/redis
      initContainers:
        - name: 0chain-redis-setup
          image: cassandra:3.11.4
          command:
            - /bin/sh
            - -ce
            - |
              mkdir -p /0chain/data/redis/transactions
              chmod -R 777 /0chain/data/redis
              mkdir -p /0chain/data/redis/state
              chmod -R 777 /0chain/data/redis
          volumeMounts:
            - name: data
              mountPath: /0chain/data
      volumes:
        - name: data
          emptyDir: {}
        - name: config
          configMap:
            name: RELEASE-NAME-0chain
        - name: redis-config
          configMap:
            name: RELEASE-NAME-0chain-redis
---
# Source: 0chain/templates/miner-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: RELEASE-NAME-0chain-miner-3
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: miner
    app.kubernetes.io/part-of: miner-3
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: 0chain
      app.kubernetes.io/instance: RELEASE-NAME
      app.kubernetes.io/component: miner
      app.kubernetes.io/part-of: miner-3
  template:
    metadata:
      labels:
        app.kubernetes.io/name: 0chain
        app.kubernetes.io/instance: RELEASE-NAME
        app.kubernetes.io/component: miner
        app.kubernetes.io/part-of: miner-3
    spec:
      serviceAccountName: RELEASE-NAME-0chain
      securityContext:
        {}
      containers:
        - name: 0chain
          securityContext:
            {}
          image: "0chaindev/miner:latest"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -ce
            - |
              mkdir -p /0chain/data/rocksdb &&
              while ! nc -z 127.0.0.1 6379; do echo "waiting for redis state service to be up"; sleep 1; done &&
              while ! nc -z 127.0.0.1 6479; do echo "waiting for redis transaction service to be up"; sleep 1; done &&
              ./bin/miner --deployment_mode 0 --keys_file config/b0mnode3_keys.txt
          env:
            - name: DOCKER
              value: ""
            - name: REDIS_HOST
              value: 127.0.0.1
            - name: REDIS_TXNS
              value: 127.0.0.1
          ports:
            - name: http
              containerPort: 7071
              protocol: TCP
          # livenessProbe:
            # httpGet:
              # path: /
              # port: http
          # readinessProbe:
            # httpGet:
              # path: /
              # port: http
          resources:
            {}
          volumeMounts:
            - name: data
              mountPath: /0chain/data
            - name: config
              mountPath: /0chain/config
        - name: 0chain-redis
          image: redis:alpine
          args:
            - redis-server
            - /0chain/config/redis/state.redis.conf
          volumeMounts:
            - name: data
              mountPath: /0chain/data
            - name: redis-config
              mountPath: /0chain/config/redis
        - name: 0chain-redis-txns
          image: redis:alpine
          args:
            - redis-server
            - /0chain/config/redis/transactions.redis.conf
          volumeMounts:
            - name: data
              mountPath: /0chain/data
            - name: redis-config
              mountPath: /0chain/config/redis
      initContainers:
        - name: 0chain-redis-setup
          image: cassandra:3.11.4
          command:
            - /bin/sh
            - -ce
            - |
              mkdir -p /0chain/data/redis/transactions
              chmod -R 777 /0chain/data/redis
              mkdir -p /0chain/data/redis/state
              chmod -R 777 /0chain/data/redis
          volumeMounts:
            - name: data
              mountPath: /0chain/data
      volumes:
        - name: data
          emptyDir: {}
        - name: config
          configMap:
            name: RELEASE-NAME-0chain
        - name: redis-config
          configMap:
            name: RELEASE-NAME-0chain-redis
---
# Source: 0chain/templates/sharder-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: RELEASE-NAME-0chain-sharder-0
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: sharder
    app.kubernetes.io/part-of: sharder-0
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: 0chain
      app.kubernetes.io/instance: RELEASE-NAME
      app.kubernetes.io/component: sharder
      app.kubernetes.io/part-of: sharder-0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: 0chain
        app.kubernetes.io/instance: RELEASE-NAME
        app.kubernetes.io/component: sharder
        app.kubernetes.io/part-of: sharder-0
    spec:
      serviceAccountName: RELEASE-NAME-0chain
      securityContext:
        {}
      containers:
        - name: 0chain
          securityContext:
            {}
          image: "0chaindev/sharder:latest"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -ce
            - |
              mkdir -p /0chain/data/blocks
              mkdir -p /0chain/data/rocksdb
              ./bin/sharder --deployment_mode 0 --keys_file config/snode0_keys.txt --minio_file config/minio_config.txt
          env:
            - name: CASSANDRA_CLUSTER
              value: RELEASE-NAME-0chain-cassandra-0.RELEASE-NAME-0chain-cassandra
            - name: DOCKER
              value: "true"
          ports:
            - name: http
              containerPort: 7171
              protocol: TCP
          # livenessProbe:
            # httpGet:
              # path: /
              # port: http
          # readinessProbe:
            # httpGet:
              # path: /
              # port: http
          resources:
            {}
          volumeMounts:
            - name: data
              mountPath: /0chain/data
            - name: config
              mountPath: /0chain/config
      initContainers:
        - name: 0chain-cassandra-setup
          image: cassandra:3.11.4
          command:
            - /bin/sh
            - -ce
            - |
              apt update || true && apt install netcat wget -y
              while ! nc -z RELEASE-NAME-0chain-cassandra-0.RELEASE-NAME-0chain-cassandra 9042; do echo "waiting for cassandra service to be up"; sleep 1; done
              echo "cassandra service is up done" && echo "downloading sql files"
              wget https://raw.githubusercontent.com/0chain/0chain/master/sql/zerochain_keyspace.sql
              wget https://raw.githubusercontent.com/0chain/0chain/master/sql/magic_block_map.sql
              wget https://raw.githubusercontent.com/0chain/0chain/master/sql/txn_summary.sql
              echo "Running migrations"
              cqlsh -f $PWD/zerochain_keyspace.sql RELEASE-NAME-0chain-cassandra-0.RELEASE-NAME-0chain-cassandra
              cqlsh -f $PWD/magic_block_map.sql RELEASE-NAME-0chain-cassandra-0.RELEASE-NAME-0chain-cassandra
              cqlsh -f $PWD/txn_summary.sql RELEASE-NAME-0chain-cassandra-0.RELEASE-NAME-0chain-cassandra
              echo "Done running migrations"
      volumes:
        - name: data
          emptyDir: {}
        - name: config
          configMap:
            name: RELEASE-NAME-0chain
---
# Source: 0chain/templates/validator-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: RELEASE-NAME-0chain-validator-0
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: validator
    app.kubernetes.io/part-of: validator-0
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: 0chain
      app.kubernetes.io/instance: RELEASE-NAME
      app.kubernetes.io/component: validator
      app.kubernetes.io/part-of: validator-0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: 0chain
        app.kubernetes.io/instance: RELEASE-NAME
        app.kubernetes.io/component: validator
        app.kubernetes.io/part-of: validator-0
    spec:
      serviceAccountName: RELEASE-NAME-0chain
      securityContext:
        fsGroup: 999
        runAsUser: 999
      containers:
        - name: 0chain
          securityContext:
            capabilities:
              add:
              - IPC_LOCK
          image: "0chaindev/validator:latest"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - ./bin/validator --port "5051" --hostname v1.load.testnet-0chain.net --deployment_mode "0"
              --keys_file config/b0bnode0_keys.txt --log_dir /blobber/log
          env:
            - name: DOCKER
              value: "true"
          ports:
            - name: http5051
              containerPort: 5051
              protocol: TCP
          resources:
            {}
          volumeMounts:
            - name: data
              mountPath: /0chain/log
            - mountPath: /blobber/config/
              name: validator-config
      volumes:
        - name: data
          emptyDir: {}
        - name: validator-config
          configMap:
            name: RELEASE-NAME-0chain-blobber
---
# Source: 0chain/templates/cassandra-sts.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: RELEASE-NAME-0chain-cassandra
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: cassandra
spec:
  serviceName: RELEASE-NAME-0chain-cassandra
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: 0chain
      app.kubernetes.io/instance: RELEASE-NAME
      app.kubernetes.io/component: RELEASE-NAME-0chain-cassandra
  template:
    metadata:
      labels:
        app.kubernetes.io/name: 0chain
        app.kubernetes.io/instance: RELEASE-NAME
        app.kubernetes.io/component: RELEASE-NAME-0chain-cassandra
    spec:
      securityContext:
        fsGroup: 999
        runAsUser: 999
      containers:
        - name: 0chain
          securityContext:
            capabilities:
              add:
              - IPC_LOCK
          image: "cassandra:3.11.4"
          imagePullPolicy: IfNotPresent
          env:
            - name: MAX_HEAP_SIZE
              value: 512M
            - name: HEAP_NEWSIZE
              value: 100M
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          # livenessProbe:
            # exec:
              # command:
                # - /bin/bash
                # - -ec
                # - |
                  # nodetool status
            # failureThreshold: 6
            # periodSeconds: 10
            # successThreshold: 1
            # timeoutSeconds: 4
          resources:
            limits:
              cpu: 900m
              memory: 1500Mi
            requests:
              cpu: 700m
              memory: 1500Mi
          volumeMounts:
          - name: data
            mountPath: /var/lib/cassandra/data
      volumes:
        - name: data
          emptyDir: {}
---
# Source: 0chain/templates/mongo.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: RELEASE-NAME-0chain-mongo
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongo
spec:
  serviceName: RELEASE-NAME-0chain-mongo
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: 0chain
      app.kubernetes.io/instance: RELEASE-NAME
      app.kubernetes.io/component: RELEASE-NAME-0chain-mongo
  template:
    metadata:
      labels:
        app.kubernetes.io/name: 0chain
        app.kubernetes.io/instance: RELEASE-NAME
        app.kubernetes.io/component: RELEASE-NAME-0chain-mongo
    spec:
      securityContext:
        {}
      containers:
        - name: 0chain
          securityContext:
            {}
          image: "mongo:latest"
          imagePullPolicy: IfNotPresent
          command:
            - mongod
            - "--bind_ip"
            - 0.0.0.0
            - "--replSet"
            - rs0
          resources:
            {}
          volumeMounts:
          - name: data
            mountPath: /data/db
      volumes:
        - name: data
          emptyDir: {}
---
# Source: 0chain/templates/postgres-sts.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: RELEASE-NAME-0chain-postgres
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: postgres
spec:
  serviceName: RELEASE-NAME-0chain-postgres
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: 0chain
      app.kubernetes.io/instance: RELEASE-NAME
      app.kubernetes.io/component: RELEASE-NAME-0chain-postgres
  template:
    metadata:
      labels:
        app.kubernetes.io/name: 0chain
        app.kubernetes.io/instance: RELEASE-NAME
        app.kubernetes.io/component: RELEASE-NAME-0chain-postgres
    spec:
      securityContext:
        fsGroup: 999
        runAsUser: 999
      containers:
        - name: 0chain-postgres
          securityContext:
            capabilities:
              add:
              - IPC_LOCK
          image: "postgres:11"
          imagePullPolicy: IfNotPresent
          env:
            - name: POSTGRES_HOST_AUTH_METHOD
              value: trust
          resources:
            {}
          volumeMounts:
          - name: data
            mountPath: /var/lib/postgres/data
      volumes:
        - name: data
          emptyDir: {}
---
# Source: 0chain/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: RELEASE-NAME-0chain
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/use-regex: "true"
spec:
  rules:
    - host: "v1.load.testnet-0chain.net"
      http:
        paths:
          - path: /dns/(|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: RELEASE-NAME-0chain-0dns
                port:
                  number: 9091
          - path: /sharder0/(|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: RELEASE-NAME-0chain-sharder-0
                port:
                  number: 7171
          - path: /miner0/(|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: RELEASE-NAME-0chain-miner-0
                port:
                  number: 7071
          - path: /miner1/(|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: RELEASE-NAME-0chain-miner-1
                port:
                  number: 7071
          - path: /miner2/(|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: RELEASE-NAME-0chain-miner-2
                port:
                  number: 7071
          - path: /miner3/(|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: RELEASE-NAME-0chain-miner-3
                port:
                  number: 7071
          - path: /blobber0/(|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: RELEASE-NAME-0chain-blobber-0
                port:
                  number: 3130
          - path: /blobber1/(|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: RELEASE-NAME-0chain-blobber-1
                port:
                  number: 3130
          - path: /validator0/(|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: RELEASE-NAME-0chain-validator-0
                port:
                  number: 5051
---
# Source: 0chain/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "RELEASE-NAME-0chain-test-connection-sharder-0"
  labels:
    helm.sh/chart: 0chain-0.1.0
    app.kubernetes.io/name: 0chain
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['RELEASE-NAME-0chainsharder-0:7171']
  restartPolicy: Never
